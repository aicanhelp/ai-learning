## 混合AI：
混合AI是AI的未来，将支持生成式AI应用开发者和提供商利用边缘侧终端的计算能力降低成本。混合AI架构或仅在终端侧运行AI，能够在全球范围带来高性能、个性化、隐私和安全等优势。此外，混合AI架构可以根据模型和查询需求的复杂度等因素，选择不同方式在云端和终端侧之间分配处理负载。
在混合AI场景中，边缘大模型是云端大模型的感知器官，例如用户对手机说话，自动语音识别（ASR）AI模型如Whisper在设备上转换语音为文本，发送到云端，云端运行大模型，回发文本答案。

## 传统端侧推理：
- MobileNet：将传统conv替换为DWconv与PWconv，降低Flops，DW+PW = 传统conv，但是所需要的Flops更低，在论文中，V3Flops低于V2，且效果更好一些；
- octConv： 将传统conv替换为octConv，可以进一步降低Flops，其原理为卷积得到的特征中，存在高频特征与低频特征，而低频特征在卷积时可以降低分辨率，在论文中，ocv-mobilenet其Flops更低，且效果略好一些；
- GhostNet：将传统conv替换为ghost module，可以降低Flops，其原理特征映射之间的相关性与冗余性，即部分特征可以直接通过对其他特征进行简单的线性变换得到，而不用进行卷积；
- SkipNet：顾名思义，即认为多层的cnn，是为了解决部分cornercase而设计的，一部分case并不需要走完网络全程，所以可以通过“跳跃”的形式来提高性能；
- MobileVit：针对Mobile优化的VIT，其为transformer，并非传统cnn，故这里并没有多研究；
- TineNet： 其主要阐述了一个观点，即在轻量级网络上面，对于模型效果影响最多的因素，主要为分辨率>网络深度>网络宽度。

## 访存问题：
现在的AI推理，算力往往不是瓶颈，很多时候被访存性能卡住的，移动端的访存带宽其实并不高，按照8450 GPU，3T的fp16算力，mobilnet才0.5G，计算部分0.16ms的事情，但实际要3-5ms；还有CPU和GPU之间的数据拷贝，那个性能更慢，10G/s附近，折算成ms，是10M/ms，在实际相机中，GPU通常会被各个模块占用，实际给模型能用的就更少了。

## 分析结论：
计算量并不能单独用来评估模型的推理时间，还必须结合硬件特性（算力&带宽），以及访存量来进行综合评估。
除上述因素外，系统环境也会对性能产生影响，比如GPU或者NPU在某些条件下会降频。
软件实现上也会影响性能，即底层框架对于算子的优化力度，比如空洞卷积（dilated Conv）性能会弱于普通卷积的原因是前者对访存的利用不如后者高效。
功耗层面，当前并没有可以直接预估的方式，必须要实测。
性能上，可以基于上述分析结果指导优化，但是有条件的情况下最好还是实测。

## 对于优化的指导：
对于高算力平台（GPU、DSP 等），一味降低计算量来降低推理时间就并不可取了，往往更需要关注访存量。
单纯降低计算量，很容易导致网络落到硬件的访存密集区，导致推理时间与计算量不成线性关系，反而跟访存量呈强相关（而这类硬件往往内存弱于计算）。
相对于低计算密度网络而言，高计算密度网络有可能因为硬件效率更高，耗时不变乃至于更短。
面向推理性能设计网络结构时，尽量采用经典结构，大部分框架会对这类结构进行图优化，能够有效减少计算量与访存量。
例如 Conv->BN->ReLU 就会融合成一个算子，但 Conv->ReLU->BN 就无法直接融合 BN 层
算子的参数尽量使用常用配置，如 Conv 尽量使用 3x3_s1/s2、1×1_s1/s2 等，软件会对这些特殊参数做特殊优化。
CNN 网络 channel 数尽量选择 4/8/16/32 的幂次，很多框架的很多算子实现在这样的 channel 数下效果更好

主要优化方向图优化，减少计算量与访存量

## 大模型轻量化：
 （1）量化感知训练（Quantization-Aware Training，QAT）
 （2）量化感知微调（Quantization-Aware Fine-tuning，QAF）
 （3）训练后量化（Post-Training Quantization，PTQ）

llama.cpp  int 4 的量化，实现 苹果手机 llama 7B的推理
针对量化精度下降，一是场景的接受程度，二是通过大小模型混合的方式，提升总体性能

## 大模型蒸馏：
（1）Google研究人员与华盛顿大学研究人员提出新的模型蒸馏方法：逐步蒸馏法详解，即逐步蒸馏（Distilling step-by-step）法

## 专用AI芯片：
（1）地平线的BPU大模型推理芯片
（2）联发科、高通等AI芯片

## 混合AI：
 （1）方向一：大模型在边端，小模型在端。这种模式，可以把小模型比作是大模型的一个微调。
 （2）方向二：小模型在端，大模型在边。边端完成推理前的数据预处理，边端做大模型推理。比如语音转文本在端侧，大模型在边侧做推理

## 减少计算量与访存量：
    主要是针对芯片的体系结构和模型网络结构是优化。包括利用图优化技术，做编译优化。
    （1）充分利用高速缓存
    （2）根据模型模型的网络结果，做计算调整，比如一些计算顺序的调整，充分发挥芯片计算能力
            通常算法人员只是按照现有算子去实现网络，并没有结合芯片本身的计算特性去充分优化

## 手机端大模型应用：
   （1）手机应用Agent，帮助用户更好的使用手机各种工具APP
   （2）

## AI终端设备：
Humane凭借Ai Pin吸引人们注意力的同时，其他竞品也在路上，目前广受关注的便携式AI产品还包括Rewind Pendant、Tab、Meta的Ray-Ban等。它们形态各异，比如Rewind Pendant看起来很像一个挂在项链上的吊坠，已开启售卖；Ray-Ban是一副眼镜，已经推出了第二代；Tab像一个小型溜溜球，将于2024 年冬季或春季首次亮相。

## 热门大模型应用：
在ChatGPT等通用大模型之外，Midjourney、Character.AI、Github Copilot、Pika推出图片生产、角色扮演、代码助手、视频生成等服务，部分应用已拥有较强的用户黏性

## Agent：
大模型不能万能的，不是在任何领域都能SOTA。大模型的高明之处是不是那么直。它可以利用其他能力和工具来提升自己的能力上限。
Agent可以理解为一种能够自主理解、规划和执行复杂任务的系统。
Model --> Prompt --> Chain --> Agent --> Multi Agent
Agent = 大型模型 + 记忆 + 主动规划 + 工具使用

## 应用设想：
（1）娱乐： 博客、短视频、剪辑、配乐、广告、游戏剧本设计、
（2）文案： 框架、提纲、摘要
（3）研发： 需求文档、设计文档、数据库设计、研发计划、工程框架、软件框架、单元测试
（4）助手： 

