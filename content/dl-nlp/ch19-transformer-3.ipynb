{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer\n",
    "\n",
    "Apr 3, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/aiayn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "a[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_0_0.png)\n",
    "\n",
    "在过去的一年中，[“注意就是您所需要的一切”中的“](https://arxiv.org/abs/1706.03762)变形金刚[”](https://arxiv.org/abs/1706.03762)已经引起了很多人的关注。除了在翻译质量上取得重大改进外，它还为其他许多NLP任务提供了新的体系结构。论文本身写得很清楚，但是传统的看法是，很难正确实施。\n",
    "\n",
    "在这篇文章中，我以逐行实施的形式介绍了本文的“注释”版本。我已经重新排序并从原始论文中删除了一些部分，并在全文中添加了评论。该文档本身是一个有效的笔记本，应完全可用。总共有400行库代码，可在4个GPU上每秒处理27,000个令牌。\n",
    "\n",
    "要继续进行，您首先需要安装 [PyTorch](http://pytorch.org/)。完整的笔记本也可以在 [github](https://github.com/harvardnlp/annotated-transformer)或带有免费GPU的Google [Colab上](https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view?usp=sharing)获得。\n",
    "\n",
    "请注意，这仅仅是研究人员和感兴趣的开发人员的起点。此处的代码主要基于我们的[OpenNMT](http://opennmt.net/)软件包。（如果有帮助，请随时[引用](http://nlp.seas.harvard.edu/2018/04/03/attention.html#conclusion)。）对于模型检出的其他全服务实现， [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)（tensorflow）和 [Sockeye](https://github.com/awslabs/sockeye)（mxnet）。\n",
    "\n",
    "- 亚历山大·拉什（[@harvardnlp](https://twitter.com/harvardnlp)或srush@seas.harvard.edu），在Vincent Nguyen和Guillaume Klein的帮助下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 目录\n",
    ">\n",
    "> - [预赛](http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims)\n",
    "> - [背景](http://nlp.seas.harvard.edu/2018/04/03/attention.html#background)\n",
    "> - 模型架构\n",
    ">   - 编码器和解码器堆栈\n",
    ">     - [编码器](http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder)\n",
    ">     - [解码器](http://nlp.seas.harvard.edu/2018/04/03/attention.html#decoder)\n",
    ">     - [注意](http://nlp.seas.harvard.edu/2018/04/03/attention.html#attention)\n",
    ">     - [注意在我们模型中的应用](http://nlp.seas.harvard.edu/2018/04/03/attention.html#applications-of-attention-in-our-model)\n",
    ">   - [位置前馈网络](http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks)\n",
    ">   - [嵌入和Softmax](http://nlp.seas.harvard.edu/2018/04/03/attention.html#embeddings-and-softmax)\n",
    ">   - [位置编码](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding)\n",
    ">   - [全模型](http://nlp.seas.harvard.edu/2018/04/03/attention.html#full-model)\n",
    "> - 训练\n",
    ">   - [批次和遮罩](http://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking)\n",
    ">   - [训练循环](http://nlp.seas.harvard.edu/2018/04/03/attention.html#training-loop)\n",
    ">   - [培训数据和批处理](http://nlp.seas.harvard.edu/2018/04/03/attention.html#training-data-and-batching)\n",
    ">   - [硬件和时间表](http://nlp.seas.harvard.edu/2018/04/03/attention.html#hardware-and-schedule)\n",
    ">   - [优化器](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer)\n",
    ">   - 正则化\n",
    ">     - [标签平滑](http://nlp.seas.harvard.edu/2018/04/03/attention.html#label-smoothing)\n",
    "> - 第一个例子\n",
    ">   - [综合数据](http://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data)\n",
    ">   - [损失计算](http://nlp.seas.harvard.edu/2018/04/03/attention.html#loss-computation)\n",
    ">   - [贪婪解码](http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding)\n",
    "> - 真实的例子\n",
    ">   - [资料载入](http://nlp.seas.harvard.edu/2018/04/03/attention.html#data-loading)\n",
    ">   - [迭代器](http://nlp.seas.harvard.edu/2018/04/03/attention.html#iterators)\n",
    ">   - [多GPU训练](http://nlp.seas.harvard.edu/2018/04/03/attention.html#multi-gpu-training)\n",
    ">   - [训练系统](http://nlp.seas.harvard.edu/2018/04/03/attention.html#training-the-system)\n",
    "> - [附加组件：BPE，搜索，平均](http://nlp.seas.harvard.edu/2018/04/03/attention.html#additional-components-bpe-search-averaging)\n",
    "> - 结果\n",
    ">   - [注意可视化](http://nlp.seas.harvard.edu/2018/04/03/attention.html#attention-visualization)\n",
    "> - [结论](http://nlp.seas.harvard.edu/2018/04/03/attention.html#conclusion)\n",
    ">\n",
    "> > 我的评论被引用了。正文全部来自论文本身。\n",
    "\n",
    "# 背景\n",
    "\n",
    "减少顺序计算的目标也构成了扩展神经GPU，ByteNet和ConvS2S的基础，它们全部使用卷积神经网络作为基本构建块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数在位置之间的距离中增加，对于ConvS2S线性增长，而对于ByteNet则对数增长。这使得学习远处位置之间的依赖性变得更加困难。在“transformer”中，此操作被减少为恒定的操作次数，尽管这是由于平均注意力加权位置而导致有效分辨率降低的代价，我们用多头注意力来抵消这种效果。\n",
    "\n",
    "自我注意，有时也称为内部注意，是一种与单个序列的不同位置相关的注意力机制，目的是计算序列的表示形式。自我注意已成功用于各种任务中，包括阅读理解，抽象总结，文本蕴涵和学习与任务无关的句子表示。端到端内存网络基于递归注意机制，而不是序列对齐的递归，并且已被证明在简单语言问答和语言建模任务中表现良好。\n",
    "\n",
    "据我们所知，Transformer是第一个完全依靠自我注意力来计算其输入和输出表示的转导模型，而无需使用序列对齐的RNN或卷积。\n",
    "\n",
    "# 模型架构\n",
    "\n",
    "大多数竞争性神经序列转导模型具有编码器-解码器结构[（引用）](https://arxiv.org/abs/1409.0473)。在此, 编码器映射一个符号表示的输入序列 (x1,…,xn)(x1,…,xn) 到一个连续表示的序列 **z**=(z1,…,zn)z=(z1,…,zn). 通过 **z**z, 解码器按一次一个符号的方式生产一个输出序列 (y1,…,ym)(y1,…,ym) . 模型的每一步都是自动回归的 [（引用）](https://arxiv.org/abs/1308.0850)，在生成下一个时，会将先前生成的符号用作附加输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变压器遵循这种总体架构，对编码器和解码器使用堆叠式自注意力和逐点，全连接层，分别如图1的左半部分和右半部分所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/ModalNet-21.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_14_0.png)\n",
    "\n",
    "## 编码器和解码器堆栈\n",
    "\n",
    "### 编码器\n",
    "\n",
    "编码器由相同的N=6的层栈构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们 在两个子层的每一层周围都采用了残余连接[（引用）](https://arxiv.org/abs/1512.03385)，然后进行层归一化 [（引用）](https://arxiv.org/abs/1607.06450)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说, 每个子层的输出 is $LayerNorm (x+\\text { Sublayer }(x))$, 在这里 $Sublayer (x)$ is  sub-layer 自己实现的函数. 我们应用差 [（引用）](http://jmlr.org/papers/v15/srivastava14a.html)到每个子层的输出，然后将其加入到该子层的输入和归一化。\n",
    "\n",
    "为了促进这些残留连接，模型中的所有子层以及嵌入层均产生尺寸为$d_{\\text {model}}=512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每层都有两个子层。第一个是多头自我关注机制，第二个是简单的位置完全连接的前馈网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器\n",
    "\n",
    "编码器也是由相同的$N=6$的层栈构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了每个编码器层中的两个子层之外，解码器还插入第三子层，该第三子层对编码器堆栈的输出执行多头关注。与编码器类似，我们在每个子层周围采用残余连接，然后进行层归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还修改了解码器堆栈中的自我注意子层，以防止位置关注后续位置。这种掩盖结合输出嵌入偏移一个位置这一事实，可以确保对位置i的预测只能取决于位置小于i的已知输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False],\n",
       "         [ True, False, False],\n",
       "         [ True,  True, False]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.triu(np.ones((1,3,3)))\n",
    "torch.from_numpy(a)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *注意框下方显示每个tgt单词（行）允许查看的位置（列）。在训练期间，由于将来出现的单词而使单词被阻止。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAE5CAYAAAAQtqIuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_31_0.png)\n",
    "\n",
    "### 注意\n",
    "\n",
    "注意功能可以描述为将查询和一组键值对映射到输出，其中查询，键，值和输出都是向量。将输出计算为值的加权总和，其中分配给每个值的权重是通过查询与相应键的兼容性函数来计算的。\n",
    "\n",
    "我们将我们的特别关注称为“点状产品关注度”。输入包含维度为d的查询和键$d_{k}$，以及维度d的值$d_{v}$。我们用所有键计算查询的点积，将每个键除以 $\\sqrt{d_{k}}$，然后应用softmax函数来获取值的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/ModalNet-19.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_33_0.png)\n",
    "\n",
    "在实际应用中，我们同时计算一组查询的注意函数，并将其组合成一个矩阵$Q$。键和值也被打包成矩阵$K$和$V$。我们将输出矩阵计算为：\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个最常用的注意力功能是加性注意力 [（引用）](https://arxiv.org/abs/1409.0473)和点乘（乘法）注意力。点积注意与我们的算法相同，只是比例因子为$\\frac{1}{\\sqrt{d_{k}}}$。加性注意事项使用具有单个隐藏层的前馈网络来计算兼容性函数。尽管两者在理论上的复杂度相似，但是在实践中点积的关注要快得多，并且空间效率更高，因为可以使用高度优化的矩阵乘法代码来实现。\n",
    "\n",
    "而对于较小的$d_{k}$两种机制的表现相似，加性注意的效果优于点积的注意，而无需针对较大的$d$进行缩放 $d_{k}$ [（引用）](https://arxiv.org/abs/1703.03906)。我们怀疑对于较大的$d_{k}$，点积的大小会增大，从而将$softmax$函数推入梯度极小的区域（为说明点积变大的原因，请假设$q$的分量$q$和$k$是均值为0的独立随机变量0和方差1个。于是他们点的产品，$q \\cdot k=\\sum_{i=1}^{d k} q_{i} k_{i}$平均值为0和方差 $d_{k}$。为了抵消这种影响，我们将点积缩放$\\frac{1}{\\sqrt{d_{k}}}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/ModalNet-20.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_38_0.png)\n",
    "\n",
    "多头注意力使模型可以共同关注来自不同位置的不同表示子空间的信息。 对于一个注意力集中的头部，平均抑制了这一点。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { MultiHead( }Q, K, V)=\\text { Concat (head }_{1}, \\ldots, \\text { head }_{\\mathrm{h}} \\text { ) } W^{O}\\\\\n",
    "&\\text { where head }_{\\mathrm{i}}=\\text { Attention }\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这里投影是参数矩阵 $W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text {model }} d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{\\text {model }} d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{\\text {model}} d_{v}} \\text { and } W^{O} \\in \\mathbb{R}^{h d_{v} \\times d_{\\text {model }}}\n",
    "$. 在这项工作中，我们采用$ h = 8 $平行注意层或头部。 对于每一个，我们使用$d_{k}=d_{y}=d_{\\text {model}}/h=64$。 由于每个头部的尺寸减小，因此总的计算成本类似于具有全尺寸的单头注意力的计算成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意在我们模型中的应用\n",
    "\n",
    "Transformer以三种不同的方式使用多头注意：\n",
    "\n",
    "1）在“编码器-解码器注意”层中，查询来自上一个解码器层，而存储键和值来自编码器的输出。这允许解码器中的每个位置都参与输入序列中的所有位置。这模仿了诸如[（cite）之](https://arxiv.org/abs/1609.08144)类的序列到序列模型中的典型编码器-解码器注意机制 。\n",
    "\n",
    "2）编码器包含自我注意层。在自我关注层中，所有键，值和查询都来自同一位置，在这种情况下，是编码器中上一层的输出。编码器中的每个位置都可以覆盖编码器上一层中的所有位置。\n",
    "\n",
    "3）类似地，解码器中的自我注意层允许解码器中的每个位置都参与解码器中的所有位置，直到该位置为止。我们需要防止解码器中向左流动信息，以保留自回归属性。我们通过屏蔽掉（设置来实现此的缩放点状产品关注内部-∞-∞）softmax输入中与非法连接相对应的所有值。\n",
    "\n",
    "## 位置前馈网络\n",
    "\n",
    "除了关注子层之外，我们的编码器和解码器中的每个层还包含一个完全连接的前馈网络，该网络被分别并相同地应用于每个位置。这由两个线性变换组成，两个线性变换之间具有ReLU激活。\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
    "$$\n",
    "\n",
    "虽然线性变换在不同位置上相同，但是它们使用不同的参数。 描述此问题的另一种方式是将其与内核大小为1的两个卷积进行比较。输入和输出的维数为$d_{model}=512$，而内层的维数为$d_{ff}=2048$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入和Softmax\n",
    "\n",
    "与其他序列转导模型类似，我们使用学习的嵌入将输入标记和输出标记转换为维度$d_{model}$的向量。我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一个令牌概率。在我们的模型中，我们在两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，类似于 [（cite）](https://arxiv.org/abs/1608.05859)。在嵌入层中，我们将这些权重乘以$\\sqrt{d_{\\text {model }}}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "由于我们的模型不包含重复性和卷积，因此为了使模型能够利用序列的顺序，我们必须注入一些有关令牌在序列中的相对或绝对位置的信息。为此，我们在编码器和解码器堆栈底部的输入嵌入中添加“位置编码”。位置编码的维数为 d模型作为嵌入，以便可以将两者相加。位置编码有很多选择，学习的和固定的 [（引用）](https://arxiv.org/pdf/1705.03122.pdf)。\n",
    "\n",
    "在这项工作中，我们使用不同频率的正弦和余弦函数:$PE_{(pos, 2i)}=\\sin (pos / 10000^{2i / d_{mode }})$\n",
    "\n",
    "$PE_{(pos, 2i)}=\\cos (pos / 10000^{2i / d_{mode }})$其中$pos$是位置，$i$是尺寸。 即，位置编码的每个维度对应于正弦曲线。 波长形成从$2π$到$10000⋅2π$的几何级数。 我们选择此函数是因为我们假设它会允许模型轻松学习相对位置的参加，因为对于任何固定的偏移$k$，$PE_{pos+k}$都可以表示为$PE_{pos}$.\n",
    "\n",
    "此外，我们对编码器和解码器堆栈中的嵌入和位置编码的总和应用了dropout。 对于基本模型，我们使用$P_{drop} = 0.1$的比率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.0000,  25.0000, 125.0000, 625.0001])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate: 5**[1,2,3,4]\n",
    "torch.exp(torch.Tensor([1,2,3,4])*math.log(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *在位置编码下方，将基于位置添加正弦波。对于每个维度，波的频率和偏移都不同。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_49_0.png)\n",
    "\n",
    "我们还尝试使用学习的位置嵌入 [（引用）](https://arxiv.org/pdf/1705.03122.pdf)代替，发现这两个版本产生了几乎相同的结果。我们选择正弦曲线版本是因为它可以使模型外推到比训练过程中遇到的序列更长的序列长度。\n",
    "\n",
    "## 全模型\n",
    "\n",
    "> *在这里，我们定义一个接受超参数并生成完整模型的函数。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "# Small example model.\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "本节描述了我们模型的训练方案。\n",
    "\n",
    "> *我们停下来快速介绍一下训练标准编码器-解码器模型所需的一些工具。首先，我们定义一个批处理对象，其中包含用于训练的src和目标句子，以及构造遮罩。*\n",
    "\n",
    "## 批次和遮罩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *接下来，我们创建一个通用的训练和评分功能来跟踪损失。我们传入一个通用损失计算函数，该函数还处理参数更新。*\n",
    "\n",
    "## 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 培训数据和批处理\n",
    "\n",
    "我们对标准的WMT 2014英语-德语数据集进行了培训，该数据集包含约450万个句子对。句子使用字节对编码进行编码，字节对编码具有约37000个令牌的共享源-目标词汇表。对于英语-法语，我们使用了更大的WMT 2014英语-法语数据集，该数据集由3600万个句子组成，并将标记拆分成32000个字词词汇。\n",
    "\n",
    "句子对按近似的序列长度进行批处理。每个训练批次包含一组句子对，其中包含大约25000个源标记和25000个目标标记。\n",
    "\n",
    "> *我们将使用火炬文本进行批处理。这将在下面更详细地讨论。在这里，我们在torchtext函数中创建批处理，以确保将填充到最大批处理大小的批处理大小不超过阈值（如果我们有8 gpu，则为25000）。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 硬件和时间表\n",
    "\n",
    "我们在一台装有8个NVIDIA P100 GPU的计算机上训练了模型。对于使用本文所述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基本模型进行了总共100,000步或12个小时的训练。对于我们的大型模型，步长为1.0秒。大型模型接受了300,000步（3.5天）的培训。\n",
    "\n",
    "## 优化器\n",
    "\n",
    "我们使用Adam优化器 [(cite)](https://arxiv.org/abs/1412.6980)并使用参数 $\\beta_{1}=0.9, \\beta_{2}=0.98 \\text { and } \\epsilon=10^{-9}\n",
    "$. 我们根据下面的公司在训练中改变学习率:\n",
    "$$rate=d_{model }^{-0.5} \\cdot min (step_{-} \\text {num}^{-0.5}, {step}_{-} \\text{num} \\cdot warmup_{-} \\text {steps}^{-1.5}) $$\n",
    "这对应于第一个$warmup_{s}steps$训练步中学习率的增加，以及按步数的平方根成反比减小学习率。 我们使用的 $warmup_{s}steps=4000$.\n",
    "\n",
    "> 注意：这部分非常重要。需要训练模型的这种设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *不同模型大小和优化超参数的此模型曲线的示例。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_69_0.png)\n",
    "\n",
    "## 正则化\n",
    "\n",
    "### 标签平滑\n",
    "\n",
    "在训练期间，我们采用了值$ϵ_{ls}=0.1$的标签平滑处理 [(cite)](https://arxiv.org/abs/1512.00567). 这伤害了困惑，因为模型学会了更加不确定，但提高了准确性和BLEU分数。\n",
    "\n",
    "> *我们使用KL div损失实现标签平滑。而不是使用单一目标分布，我们创建的分布具有`confidence`正确的单词，其余的`smoothing`质量分布在整个词汇表中。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `torch.union` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.Tensor([[1,2,3,4]])\n",
    "a.squeeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=set([1,2,3,4])\n",
    "b=set([2,5])\n",
    "a.union(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *在这里，我们可以看到一个示例，说明如何根据置信度将质量分配给单词。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing(5, 0, 0.4)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_74_0.png)\n",
    "\n",
    "> *如果标签平滑化对于给定的选择非常有信心，则标签平滑实际上开始对模型造成不利影响。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_76_0.png)\n",
    "\n",
    "# 第一个例子\n",
    "\n",
    "> *我们可以从尝试一个简单的复制任务开始。给定来自少量词汇的随机输入符号集，目标是产生回相同的符号。*\n",
    "\n",
    "## 综合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data[0] * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贪婪解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, \n",
    "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    model.eval()\n",
    "    print(run_epoch(data_gen(V, 30, 5), model, \n",
    "                    SimpleLossCompute(model.generator, criterion, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch Step: 1 Loss: 3.023465 Tokens per Sec: 403.074173\n",
    "Epoch Step: 1 Loss: 1.920030 Tokens per Sec: 641.689380\n",
    "1.9274832487106324\n",
    "Epoch Step: 1 Loss: 1.940011 Tokens per Sec: 432.003378\n",
    "Epoch Step: 1 Loss: 1.699767 Tokens per Sec: 641.979665\n",
    "1.657595729827881\n",
    "Epoch Step: 1 Loss: 1.860276 Tokens per Sec: 433.320240\n",
    "Epoch Step: 1 Loss: 1.546011 Tokens per Sec: 640.537198\n",
    "1.4888023376464843\n",
    "Epoch Step: 1 Loss: 1.682198 Tokens per Sec: 432.092305\n",
    "Epoch Step: 1 Loss: 1.313169 Tokens per Sec: 639.441857\n",
    "1.3485562801361084\n",
    "Epoch Step: 1 Loss: 1.278768 Tokens per Sec: 433.568756\n",
    "Epoch Step: 1 Loss: 1.062384 Tokens per Sec: 642.542067\n",
    "0.9853351473808288\n",
    "Epoch Step: 1 Loss: 1.269471 Tokens per Sec: 433.388727\n",
    "Epoch Step: 1 Loss: 0.590709 Tokens per Sec: 642.862135\n",
    "0.5686767101287842\n",
    "Epoch Step: 1 Loss: 0.997076 Tokens per Sec: 433.009746\n",
    "Epoch Step: 1 Loss: 0.343118 Tokens per Sec: 642.288427\n",
    "0.34273059368133546\n",
    "Epoch Step: 1 Loss: 0.459483 Tokens per Sec: 434.594030\n",
    "Epoch Step: 1 Loss: 0.290385 Tokens per Sec: 642.519464\n",
    "0.2612409472465515\n",
    "Epoch Step: 1 Loss: 1.031042 Tokens per Sec: 434.557008\n",
    "Epoch Step: 1 Loss: 0.437069 Tokens per Sec: 643.630322\n",
    "0.4323212027549744\n",
    "Epoch Step: 1 Loss: 0.617165 Tokens per Sec: 436.652626\n",
    "Epoch Step: 1 Loss: 0.258793 Tokens per Sec: 644.372296\n",
    "0.27331129014492034\n",
    "```\n",
    "\n",
    "> *为了简单起见，此代码使用贪婪解码来预测翻译。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    1     2     3     4     5     6     7     8     9    10\n",
    "[torch.LongTensor of size 1x10]\n",
    "```\n",
    "\n",
    "> # 真实的例子\n",
    ">\n",
    "> > *现在，我们考虑一个使用IWSLT德语-英语翻译任务的真实示例。该任务比本文中考虑的WMT任务小得多，但它说明了整个系统。我们还将展示如何使用多GPU处理使其真正快速。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 资料载入\n",
    "\n",
    "> *我们将使用torchtext和spacy加载数据集以进行标记化。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "    BLANK_WORD = \"<blank>\"\n",
    "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "    MAX_LEN = 100\n",
    "    train, val, test = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    MIN_FREQ = 2\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *批处理对速度至关重要。我们希望批次非常均匀，并且填充量绝对最少。为此，我们必须对默认的torchtext批处理有所了解。此代码修补了它们的默认批处理，以确保我们搜索足够多的句子以查找紧凑的批处理。*\n",
    "\n",
    "## 迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多GPU训练\n",
    "\n",
    "> *最后，要真正针对快速培训，我们将使用multi-gpu。该代码实现了多GPU字生成。它不是特定于变压器的，因此我不会赘述。想法是在训练时将单词生成分为多个块，以在许多不同的GPU上并行处理。我们使用pytorch并行原语进行此操作：*\n",
    "\n",
    "- 复制-将模块拆分到不同的GPU上。\n",
    "- 分散-将批次拆分到不同的GPU\n",
    "- parallel_apply-将模块应用于不同GPU上的批处理\n",
    "- 收集-将分散的数据拉回到一个GPU上。\n",
    "- nn.DataParallel-一种特殊的模块包装器，在评估之前将其全部调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not interested in multigpu.\n",
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
    "        # Send out to different gpus.\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate(criterion, \n",
    "                                               devices=devices)\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def __call__(self, out, targets, normalize):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate(self.generator, \n",
    "                                                devices=self.devices)\n",
    "        out_scatter = nn.parallel.scatter(out, \n",
    "                                          target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets, \n",
    "                                      target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # Predict distributions\n",
    "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
    "                                    requires_grad=self.opt is not None)] \n",
    "                           for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            # Compute loss. \n",
    "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
    "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
    "                 for g, t in zip(gen, targets)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather(loss, \n",
    "                                   target_device=self.devices[0])\n",
    "            l = l.sum()[0] / normalize\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        # Backprop all loss through transformer.            \n",
    "        if self.opt is not None:\n",
    "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather(out_grad, \n",
    "                                    target_device=self.devices[0])\n",
    "            o1.backward(gradient=o2)\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return total * normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *现在我们创建模型，准则，优化器，数据迭代器和并行化*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUs to use\n",
    "devices = [0, 1, 2, 3]\n",
    "if True:\n",
    "    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "    model.cuda()\n",
    "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "    criterion.cuda()\n",
    "    BATCH_SIZE = 12000\n",
    "    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                            batch_size_fn=batch_size_fn, train=True)\n",
    "    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                            batch_size_fn=batch_size_fn, train=False)\n",
    "    model_par = nn.DataParallel(model, device_ids=devices)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *现在我们训练模型。我将进行一些预热步骤，但是其他所有操作都使用默认参数。在具有4个Tesla V100的AWS p3.8xlarge上，此速度以每秒约27,000个令牌运行，批处理大小为12,000*\n",
    "\n",
    "## 训练系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt\n",
    "if False:\n",
    "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    for epoch in range(10):\n",
    "        model_par.train()\n",
    "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "                  model_par, \n",
    "                  MultiGPULossCompute(model.generator, criterion, \n",
    "                                      devices=devices, opt=model_opt))\n",
    "        model_par.eval()\n",
    "        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "                          model_par, \n",
    "                          MultiGPULossCompute(model.generator, criterion, \n",
    "                          devices=devices, opt=None))\n",
    "        print(loss)\n",
    "else:\n",
    "    model = torch.load(\"iwslt.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *训练后，我们可以对模型进行解码，以生成一组翻译。在这里，我们只翻译验证集中的第一句话。该数据集非常小，因此带有贪婪搜索的翻译相当准确。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(valid_iter):\n",
    "    src = batch.src.transpose(0, 1)[:1]\n",
    "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "    out = greedy_decode(model, src, src_mask, \n",
    "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
    "    print(\"Translation:\", end=\"\\t\")\n",
    "    for i in range(1, out.size(1)):\n",
    "        sym = TGT.vocab.itos[out[0, i]]\n",
    "        if sym == \"</s>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    print(\"Target:\", end=\"\\t\")\n",
    "    for i in range(1, batch.trg.size(0)):\n",
    "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
    "        if sym == \"</s>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    break\n",
    "Translation:\t<unk> <unk> . In my language , that means , thank you very much . \n",
    "Gold:\t<unk> <unk> . It means in my language , thank you very much . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附加组件：BPE，搜索，平均\n",
    "\n",
    "> 因此，这主要涵盖了变压器模型本身。我们没有明确涵盖四个方面。我们还在[OpenNMT-py中](https://github.com/opennmt/opennmt-py)实现了所有这些附加功能。\n",
    "\n",
    "> 1）BPE /词段：我们可以使用一个库来将数据首先预处理为子词单元。请参阅Rico Sennrich的子[词nmt](https://github.com/rsennrich/subword-nmt)实现。这些模型将转换训练数据，如下所示：\n",
    "\n",
    "▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .\n",
    "\n",
    ">  2）共享嵌入：当使用共享词汇的BPE时，我们可以在源/目标/生成器之间共享相同的权重向量。有关详细信息，请参见 [（引用）](https://arxiv.org/abs/1608.05859)。要将其添加到模型中，只需执行以下操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\n",
    "    model.generator.lut.weight = model.tgt_embed[0].lut.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3）光束搜索：这有点太复杂，无法在此处进行介绍。有关 pytorch的实现，请参见[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/Beam.py)。\n",
    "\n",
    "> 4）模型平均：本文将最后k个检查点平均，以产生集合效果。如果有很多模型，我们可以在事后执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(model, models):\n",
    "    \"Average models into model\"\n",
    "    for ps in zip(*[m.params() for m in [model] + models]):\n",
    "        p[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果\n",
    "\n",
    "在WMT 2014英语到德语的翻译任务中，大型变压器模型（表2中的变压器（大））比以前报告的最佳模型（包括合奏）高出2.0 BLEU，建立了新的状态art BLEU得分为28.4。表3的底部列出了该模型的配置。在8个P100 GPU上进行了3.5天的培训。甚至我们的基本模型都超过了以前发布的所有模型和集合，而其培训成本只是任何竞争模型的一小部分。\n",
    "\n",
    "在2014年WMT英语到法语翻译任务中，我们的大型模型的BLEU得分达到41.0，优于以前发布的所有单个模型，而培训费用不到以前的最新水平的1/4模型。为英语到法语训练的Transformer（大型）模型使用的辍学率Pdrop = 0.1，而不是0.3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](imgs/the-annotated-transformer_113_0.png)\n",
    "\n",
    "> 我们在此处编写的代码是基本模型的一个版本。这里有此系统的完整培训版本 [（示例模型）](http://opennmt.net/Models-py/)。\n",
    ">\n",
    "> 通过上一节中的其他扩展，在EN-DE WMT上，OpenNMT-py复制达到26.9。在这里，我已将这些参数加载到我们的重新实现中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-models/en-de-model.pt\n",
    "model, SRC, TGT = torch.load(\"en-de-model.pt\")\n",
    "model.eval()\n",
    "sent = \"▁The ▁log ▁file ▁can ▁be ▁sent ▁secret ly ▁with ▁email ▁or ▁FTP ▁to ▁a ▁specified ▁receiver\".split()\n",
    "src = torch.LongTensor([[SRC.stoi[w] for w in sent]])\n",
    "src = Variable(src)\n",
    "src_mask = (src != SRC.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "out = greedy_decode(model, src, src_mask, \n",
    "                    max_len=60, start_symbol=TGT.stoi[\"<s>\"])\n",
    "print(\"Translation:\", end=\"\\t\")\n",
    "trans = \"<s> \"\n",
    "for i in range(1, out.size(1)):\n",
    "    sym = TGT.itos[out[0, i]]\n",
    "    if sym == \"</s>\": break\n",
    "    trans += sym + \" \"\n",
    "print(trans)\n",
    "Translation:\t<s> ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意可视化\n",
    "\n",
    "> *即使使用贪婪的解码器，翻译效果也相当不错。我们可以进一步对其进行可视化，以查看关注的每一层都在发生什么*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "tgt_sent = trans.split()\n",
    "def draw(data, x, y, ax):\n",
    "    seaborn.heatmap(data, \n",
    "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
    "                    cbar=False, ax=ax)\n",
    "    \n",
    "for layer in range(1, 6, 2):\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    print(\"Encoder Layer\", layer+1)\n",
    "    for h in range(4):\n",
    "        draw(model.encoder.layers[layer].self_attn.attn[0, h].data, \n",
    "            sent, sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "    \n",
    "for layer in range(1, 6, 2):\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    print(\"Decoder Self Layer\", layer+1)\n",
    "    for h in range(4):\n",
    "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(tgt_sent)], \n",
    "            tgt_sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "    print(\"Decoder Src Layer\", layer+1)\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    for h in range(4):\n",
    "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(sent)], \n",
    "            sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Encoder Layer 2\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_1.png)\n",
    "\n",
    "```\n",
    "Encoder Layer 4\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_3.png)\n",
    "\n",
    "```\n",
    "Encoder Layer 6\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_5.png)\n",
    "\n",
    "```\n",
    "Decoder Self Layer 2\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_7.png)\n",
    "\n",
    "```\n",
    "Decoder Src Layer 2\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_9.png)\n",
    "\n",
    "```\n",
    "Decoder Self Layer 4\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_11.png)\n",
    "\n",
    "```\n",
    "Decoder Src Layer 4\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_13.png)\n",
    "\n",
    "```\n",
    "Decoder Self Layer 6\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_15.png)\n",
    "\n",
    "```\n",
    "Decoder Src Layer 6\n",
    "```\n",
    "\n",
    "![png](imgs/the-annotated-transformer_119_17.png)\n",
    "\n",
    "# 结论\n",
    "\n",
    "> 希望该代码对将来的研究有用。如有任何问题，请与我们联系。如果您认为此代码有帮助，请查看我们的其他OpenNMT工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inproceedings{opennmt,\n",
    "  author    = {Guillaume Klein and\n",
    "               Yoon Kim and\n",
    "               Yuntian Deng and\n",
    "               Jean Senellart and\n",
    "               Alexander M. Rush},\n",
    "  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},\n",
    "  booktitle = {Proc. ACL},\n",
    "  year      = {2017},\n",
    "  url       = {https://doi.org/10.18653/v1/P17-4012},\n",
    "  doi       = {10.18653/v1/P17-4012}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cheers, srush"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
