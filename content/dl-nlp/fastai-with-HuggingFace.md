# Fastai with HuggingFace ğŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)



æ³¨æ„ï¼šæ­¤å®ç°æ˜¯ä¸­å‹æ–‡ç« [â€œå¸¦æœ‰ğŸ¤—Transformersï¼ˆBERTï¼ŒRoBERTaï¼ŒXLNetï¼ŒXLMï¼ŒDistilBERTï¼‰çš„Fastaiâ€çš„è¡¥å……](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376)ã€‚

# ç®€ä»‹ï¼šNLPä¸­çš„è½¬ç§»å­¦ä¹ æ•…äº‹

åœ¨2018å¹´åˆï¼ŒJeremy Howardï¼ˆfast.aiçš„å…±åŒåˆ›å§‹äººï¼‰å’ŒSebastian Ruderæ¨å‡ºäº†[ç”¨äºæ–‡æœ¬åˆ†ç±»](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf)çš„[é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒ](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf)ï¼ˆULMFiTï¼‰æ–¹æ³•ã€‚ULMFiTæ˜¯åº”ç”¨äºNLP çš„ç¬¬ä¸€ç§**è½¬ç§»å­¦ä¹ **æ–¹æ³•ã€‚ç»“æœï¼Œé™¤äº†æ˜æ˜¾èƒœè¿‡è®¸å¤šæœ€å…ˆè¿›çš„ä»»åŠ¡å¤–ï¼Œå®ƒè¿˜å…è®¸ä»…ç”¨100ä¸ªå¸¦æ ‡ç­¾çš„ç¤ºä¾‹æ¥åŒ¹é…ç­‰åŒäºä½¿ç”¨100å€ä»¥ä¸Šæ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚

æˆ‘ç¬¬ä¸€æ¬¡å¬è¯´ULMFiTæ˜¯åœ¨Jeremy Howardæä¾›çš„[fast.aiè¯¾ç¨‹ä¸­](https://course.fast.ai/videos/?lesson=4)ã€‚ç”±äº`fastai`åº“çš„åŸå› ï¼Œä»–æ¼”ç¤ºäº†ç”¨å‡ è¡Œä»£ç å®ç°å®Œæ•´çš„ULMFitæ–¹æ³•æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚åœ¨ä»–çš„æ¼”ç¤ºä¸­ï¼Œä»–ä½¿ç”¨äº†åœ¨Wikitext-103ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„AWD-LSTMç¥ç»ç½‘ç»œï¼Œå¹¶è¿…é€Ÿè·å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚ä»–è¿˜è§£é‡Šäº†å…³é”®æŠ€æœ¯ï¼ˆä¹Ÿå·²åœ¨ULMFiTä¸­è¿›è¡Œäº†æ¼”ç¤ºï¼‰ï¼Œä»¥å¾®è°ƒâ€œ **åŒºåˆ†å­¦ä¹ ç‡â€**ï¼Œâ€œ **é€æ­¥è§£å†»â€**æˆ–â€œ **å€¾æ–œä¸‰è§’å­¦ä¹ ç‡â€ç­‰æ¨¡å‹**ã€‚

è‡ªä»å¼•å…¥ULMFiTä¹‹åï¼Œ**Transfer Learning**åœ¨NLPä¸­å˜å¾—éå¸¸æµè¡Œï¼Œä½†æ˜¯Googleï¼ˆBERTï¼ŒTransformer-XLï¼ŒXLNetï¼‰ï¼ŒFacebookï¼ˆRoBERTaï¼ŒXLMï¼‰ç”šè‡³OpenAIï¼ˆGPTï¼ŒGPT-2ï¼‰éƒ½å¼€å§‹å¯¹å…¶æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨å¾ˆå¤§çš„è¯­æ–™åº“ä¸Šã€‚è¿™æ¬¡ï¼Œä»–ä»¬éƒ½æ²¡æœ‰ä½¿ç”¨AWD-LSTMç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯éƒ½ä½¿ç”¨äº†åŸºäºTransformerçš„æ›´å¼ºå¤§çš„ä½“ç³»ç»“æ„ï¼ˆè¯·[æ³¨æ„ï¼Œè¿™æ˜¯æ‚¨æ‰€éœ€è¦çš„å…¨éƒ¨](https://arxiv.org/abs/1706.03762)ï¼‰ã€‚

å°½ç®¡è¿™äº›æ¨¡å‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†æ˜¯`fastai`è¯·ä¸è¦å°†æ‰€æœ‰æ¨¡å‹éƒ½é›†æˆåœ¨ä¸€èµ·ã€‚å¹¸è¿çš„æ˜¯ï¼Œ[HuggingFaceğŸ¤—](https://huggingface.co/)åˆ›å»ºäº†ä¼—æ‰€å‘¨çŸ¥çš„[å˜å‹å™¨åº“](https://github.com/huggingface/transformers)ã€‚è¯¥åº“ä»¥å‰ç§°ä¸º`pytorch-transformers`or `pytorch-pretrained-bert`ï¼Œå®ƒæ±‡é›†äº†40å¤šç§ç»è¿‡é¢„è®­ç»ƒçš„æœ€æ–°NLPæ¨¡å‹ï¼ˆBERTï¼ŒGPT-2ï¼ŒRoBERTaï¼ŒCTRLâ€¦ï¼‰ã€‚è¯¥å®ç°æä¾›äº†æœ‰è¶£çš„å…¶ä»–å®ç”¨ç¨‹åºï¼Œä¾‹å¦‚ä»¤ç‰Œç”Ÿæˆå™¨ï¼Œä¼˜åŒ–å™¨æˆ–è°ƒåº¦ç¨‹åºã€‚

è¯¥`transformers`åº“å¯ä»¥æ˜¯è‡ªç»™è‡ªè¶³çš„ï¼Œä½†æ˜¯å°†å…¶åˆå¹¶åˆ°`fastai`åº“ä¸­å¯ä»¥æä¾›ä¸åŠŸèƒ½å¼ºå¤§çš„fastaiå·¥å…·å…¼å®¹çš„æ›´ç®€å•çš„å®ç°ï¼Œä¾‹å¦‚**åŒºåˆ†å­¦ä¹ ç‡**ï¼Œ**é€æ­¥è§£å†»**æˆ–**å€¾æ–œçš„ä¸‰è§’å½¢å­¦ä¹ ç‡**ã€‚è¿™é‡Œçš„ç›®çš„æ˜¯è®©ä»»ä½•äººï¼ˆæ— è®ºæ˜¯ä¸“å®¶è¿˜æ˜¯éä¸“å®¶ï¼‰éƒ½å¯ä»¥è½»æ¾è·å¾—æœ€æ–°ç»“æœï¼Œå¹¶â€œå†æ¬¡ä½¿NLPä¸å†é…·ç‚«â€ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå·²ç»åœ¨ä»¥ä¸‹æ–¹é¢æ¼”ç¤ºäº†HuggingFace `transformers`åº“çš„é›†æˆ`fastai`ï¼š

- Keita Kuritaçš„æ–‡ç« [â€œä½¿ç”¨Fast AIç²¾ç‚¼BERTçš„æ•™ç¨‹â€](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)ä½¿`pytorch_pretrained_bert`å›¾ä¹¦é¦†ä¸å…¼å®¹`fastai`ã€‚
- Dev Sharmaçš„æ–‡ç« [å°†RoBERTaä¸Fastaiä¸€èµ·ç”¨äºNLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)ï¼Œè¿™ä½¿`pytorch_transformers`åº“ä¸å…¼å®¹`fastai`ã€‚

å°½ç®¡è¿™äº›æ–‡ç« çš„è´¨é‡å¾ˆé«˜ï¼Œä½†å…¶æ¼”ç¤ºçš„æŸäº›éƒ¨åˆ†ä¸å†ä¸çš„æœ€æ–°ç‰ˆæœ¬å…¼å®¹`transformers`ã€‚

# ğŸ›  fastå°†å˜å‹å™¨ä¸Fastaié›†æˆä»¥è¿›è¡Œå¤šç±»åˆ†ç±»

åœ¨å¼€å§‹å®æ–½ä¹‹å‰ï¼Œè¯·æ³¨æ„å¯ä»¥ä»¥å¤šç§ä¸åŒæ–¹å¼å®Œæˆ`transformers`å†…éƒ¨é›†æˆ`fastai`ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šæä¾›æœ€é€šç”¨ï¼Œæœ€çµæ´»çš„ç®€å•è§£å†³æ–¹æ¡ˆã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘å°è¯•åœ¨ä¸¤ä¸ªåº“ä¸­è¿›è¡Œæœ€å°‘çš„ä¿®æ”¹ï¼ŒåŒæ—¶ä½¿å…¶ä¸æœ€å¤§æ•°é‡çš„è½¬æ¢å™¨ä½“ç³»ç»“æ„å…¼å®¹ã€‚

è¯·æ³¨æ„ï¼Œé™¤äº†æœ¬â€œç¬”è®°æœ¬â€å’Œâ€œ [ä¸­å‹â€æ–‡ç« å¤–](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376)ï¼Œæˆ‘è¿˜åœ¨GitHubä¸Šæä¾›äº†å¦ä¸€ä¸ªç‰ˆæœ¬ï¼ˆTODOæ·»åŠ é“¾æ¥ï¼‰ã€‚

## å›¾ä¹¦é¦†å®‰è£…

åœ¨å¼€å§‹å®æ–½ä¹‹å‰ï¼Œæ‚¨éœ€è¦å®‰è£…`fastai`å’Œ`transformers`åº“ã€‚ä¸ºæ­¤ï¼Œåªéœ€æŒ‰ç…§[æ­¤å¤„](https://github.com/fastai/fastai/blob/master/README.md#installation)å’Œ[æ­¤å¤„](https://github.com/huggingface/transformers#installation)çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚

åœ¨Kaggleä¸­ï¼Œè¯¥`fastai`åº“å·²å®‰è£…ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€è¦å®‰è£…`transformers`ï¼š

In [1]:

```python
%%bash
pip install -q transformers
```

In [2]:

```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from pathlib import Path 

import os

import torch
import torch.optim as optim

import random 

# fastai
from fastai import *
from fastai.text import *
from fastai.callbacks import *

# transformers
from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig

from transformers import BertForSequenceClassification, BertTokenizer, BertConfig
from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig
from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig
from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig
```

fastaiå’ŒTransformersåº“çš„å½“å‰ç‰ˆæœ¬åˆ†åˆ«ä¸º1.0.58å’Œ2.5.1ã€‚

In [3]:

```python
import fastai
import transformers
print('fastai version :', fastai.__version__)
print('transformers version :', transformers.__version__)
fastai version : 1.0.58
transformers version : 2.5.1
```

## ğŸ¬ taskç¤ºä¾‹ä»»åŠ¡

é€‰æ‹©çš„ä»»åŠ¡æ˜¯â€œ [ç”µå½±è¯„è®ºâ€](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview)ä¸Šçš„å¤šç±»æ–‡æœ¬åˆ†ç±»ã€‚

å¯¹äºæ¯ä¸ªæ–‡æœ¬ç”µå½±è¯„è®ºï¼Œæ¨¡å‹å¿…é¡»é¢„æµ‹æƒ…æ„Ÿçš„æ ‡ç­¾ã€‚æˆ‘ä»¬è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„åˆ†ç±»ç²¾åº¦ã€‚æƒ…æ„Ÿæ ‡ç­¾æ˜¯ï¼š

- 0â†’è´Ÿ
- 1â†’æœ‰ç‚¹æ¶ˆæ
- 2â†’ä¸­ç«‹
- 3â†’æœ‰ç‚¹ç§¯æ
- 4â†’æ­£

æ•°æ®è¢«åŠ è½½åˆ°`DataFrame`usingä¸­`pandas`ã€‚

In [4]:

```python
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip
/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip
/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv
```

In [5]:

```python
DATA_ROOT = Path("..") / "/kaggle/input/sentiment-analysis-on-movie-reviews"
train = pd.read_csv(DATA_ROOT / 'train.tsv.zip', sep="\t")
test = pd.read_csv(DATA_ROOT / 'test.tsv.zip', sep="\t")
print(train.shape,test.shape)
train.head()
```

```
(156060, 4) (66292, 3)
```

Out[5]:

|      | PhraseId | SentenceId | Phrase                                            | Sentiment |
| :--- | :------- | :--------- | :------------------------------------------------ | :-------- |
| 0    | 1        | 1          | A series of escapades demonstrating the adage ... | 1         |
| 1    | 2        | 1          | A series of escapades demonstrating the adage ... | 2         |
| 2    | 3        | 1          | A series                                          | 2         |
| 3    | 4        | 1          | A                                                 | 2         |
| 4    | 5        | 1          | series                                            | 2         |

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ•°æ®é›†ä¸­æ²¡æœ‰å•ç‹¬çš„ç”µå½±è¯„è®ºï¼Œè€Œæ˜¯ä»ä¸Šä¸‹æ–‡ä¸­å–å‡ºå¹¶åˆ†æˆè¾ƒå°éƒ¨åˆ†çš„çŸ­è¯­ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰æŒ‡å®šçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚

## ä¸»å˜å‹å™¨ç±»

åœ¨ä¸­`transformers`ï¼Œæ¯ç§æ¨¡å‹æ¶æ„éƒ½ä¸3ç§ä¸»è¦ç±»å‹çš„ç±»åˆ«ç›¸å…³è”ï¼š

- ä¸€ä¸ª**æ¨¡å‹ç±»**åˆ°åŠ è½½/å­˜å‚¨ç‰¹å®šé¢„ç³»æ¨¡å‹ã€‚
- ä¸€ä¸ª**æ ‡è®°ç”Ÿæˆå™¨ç±»**é¢„å…ˆå¤„ç†çš„æ•°æ®å¹¶ä½¿å…¶ä¸ç‰¹å®šå‹å·å…¼å®¹ã€‚
- ä¸€ä¸ª**é…ç½®ç±»ï¼Œ**ç”¨äºåŠ è½½/å­˜å‚¨ç‰¹å®šæ¨¡å‹çš„é…ç½®ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨BERTæ¶æ„è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œä½ å¯ä»¥ä½¿ç”¨[`BertForSequenceClassification`](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)çš„**æ¨¡å‹ç±»**ï¼Œ[`BertTokenizer`](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer)ç”¨äº**æ ‡è®°ç”Ÿæˆå™¨ç±»**ï¼Œå¹¶[`BertConfig`](https://huggingface.co/transformers/model_doc/bert.html#bertconfig)ä¸º**é…ç½®ç±»**ã€‚

ä¸ºäº†åœ¨ç±»ä¹‹é—´è½»æ¾åˆ‡æ¢ï¼ˆæ¯ä¸ªç±»éƒ½ä¸ç‰¹å®šçš„æ¨¡å‹ç±»å‹ç›¸å…³ï¼‰ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸ï¼Œè¯¥å­—å…¸å…è®¸é€šè¿‡ä»…æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹ç±»å‹åç§°æ¥åŠ è½½æ­£ç¡®çš„ç±»ã€‚

In [6]:

```python
MODEL_CLASSES = {
    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),
    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),
    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),
    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),
    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)
}
```

ç¨åæ‚¨å°†çœ‹åˆ°ï¼Œè¿™äº›ç±»å…±äº«ä¸€ä¸ªé€šç”¨çš„ç±»æ–¹æ³•`from_pretrained(pretrained_model_name, ...)`ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå‚æ•°`pretrained_model_name`æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¸¦æœ‰è¦åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹/ä»¤ç‰Œ/é…ç½®çš„å¿«æ·æ–¹å¼åç§°ï¼Œä¾‹å¦‚`'bert-base-uncased'`ã€‚æˆ‘ä»¬å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)çš„å˜å‹å™¨æ–‡æ¡£ä¸­æ‰¾åˆ°æ‰€æœ‰å¿«æ·æ–¹å¼åç§°ã€‚

In [7]:

```python
# Parameters
seed = 42
use_fp16 = False
bs = 16

model_type = 'roberta'
pretrained_model_name = 'roberta-base'

# model_type = 'bert'
# pretrained_model_name='bert-base-uncased'

# model_type = 'distilbert'
# pretrained_model_name = 'distilbert-base-uncased'

#model_type = 'xlm'
#pretrained_model_name = 'xlm-clm-enfr-1024'

# model_type = 'xlnet'
# pretrained_model_name = 'xlnet-base-cased'
```

In [8]:

```python
model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]
```

æ‰“å°ä¸æ‰€ä½¿ç”¨çš„`pretrained_model_name`ï¼ˆå¿«æ·æ–¹å¼åç§°ï¼‰ç›¸å¯¹åº”çš„å¯ç”¨å€¼`model_type`ã€‚

In [9]:

```python
model_class.pretrained_model_archive_map.keys()
```

Out[9]:

```python
dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬`transformers`ä»…å°†åº“ç”¨äºå¤šç±»æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ•™ç¨‹ä»…é›†æˆäº†å·²å®ç°åºåˆ—åˆ†ç±»æ¨¡å‹çš„è½¬æ¢å™¨ä½“ç³»ç»“æ„ã€‚è¿™äº›æ¨¡å‹ç±»å‹æ˜¯ï¼š

- BERTï¼ˆæ¥è‡ªGoogleï¼‰
- XLNetï¼ˆæ¥è‡ªGoogle / CMUï¼‰
- XLMï¼ˆæ¥è‡ªFacebookï¼‰
- RoBERTaï¼ˆæ¥è‡ªFacebookï¼‰
- DistilBERTï¼ˆæ¥è‡ªHuggingFaceï¼‰

ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³èµ°å¾—æ›´è¿œ-é€šè¿‡å®ç°å¦ä¸€ç§ç±»å‹çš„æ¨¡å‹æˆ–NLPä»»åŠ¡-æœ¬æ•™ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ã€‚

## å®ç”¨åŠŸèƒ½

è®¾ç½®ç§å­ä»¥ç”Ÿæˆéšæœºæ•°çš„åŠŸèƒ½ã€‚

In [10]:

```python
def seed_all(seed_value):
    random.seed(seed_value) # Python
    np.random.seed(seed_value) # cpu vars
    torch.manual_seed(seed_value) # cpu  vars
    
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value) # gpu vars
        torch.backends.cudnn.deterministic = True  #needed
        torch.backends.cudnn.benchmark = False
```

In [11]:

```python
seed_all(seed)
```

## æ•°æ®é¢„å¤„ç†

ä¸ºäº†åŒ¹é…é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»å°†æ¨¡å‹è¾“å…¥åºåˆ—æ ¼å¼åŒ–ä¸ºç‰¹å®šæ ¼å¼ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¿…é¡»å…ˆ**æ ‡è®°åŒ–**ç„¶åæ­£ç¡®**æ•°å­—**åŒ–æ–‡æœ¬ã€‚è¿™é‡Œçš„å›°éš¾åœ¨äºï¼Œæˆ‘ä»¬å°†è¿›è¡Œå¾®è°ƒçš„æ¯ä¸ªé¢„è®­ç»ƒæ¨¡å‹éƒ½éœ€è¦â€Šä¸é¢„è®­ç»ƒéƒ¨åˆ†ä¸­ä½¿ç”¨çš„é¢„å¤„ç†å®Œå…¨ç›¸åŒçš„ç‰¹å®šé¢„å¤„ç†ï¼ˆâ€Š **æ ‡è®°åŒ–**å’Œ**æ•°å­—****åŒ–**ï¼‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œ**tokenizerç±»**ä»ä¸­`transformers`æä¾›äº†ä¸æ¯ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ç›¸å¯¹åº”çš„æ­£ç¡®çš„é¢„å¤„ç†å·¥å…·ã€‚

åœ¨è¯¥`fastai`åº“ä¸­ï¼Œåˆ›å»ºæ—¶ä¼šè‡ªåŠ¨å®Œæˆæ•°æ®é¢„å¤„ç†`DataBunch`ã€‚æ­£å¦‚æ‚¨å°†åœ¨`DataBunch`å®ç°ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œ**ä»¤ç‰ŒåŒ–å™¨**å’Œ**æ•°å­—****åŒ–**å™¨ä»¥ä»¥ä¸‹æ ¼å¼åœ¨å¤„ç†å™¨å‚æ•°ä¸­ä¼ é€’ï¼š

```python
processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ†æä¸€ä¸‹å¦‚ä½•å°†`transformers` **æ ‡è®°**å™¨é›†æˆåˆ°`TokenizeProcessor`å‡½æ•°ä¸­ã€‚

### è‡ªè®¢åˆ†è¯å™¨

è¿™éƒ¨åˆ†å¯èƒ½ä¼šé€ æˆä¸€äº›æ··ä¹±ï¼Œå› ä¸ºè®¸å¤šç±»ç›¸äº’åŒ…è£¹å¹¶ä¸”åç§°ç›¸ä¼¼ã€‚ç»§ç»­ï¼Œå¦‚æœæˆ‘ä»¬ä»”ç»†åœ°çœ‹ä¸€ä¸‹`fastai`å®ç°ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ï¼š

1. è¯¥[`TokenizeProcessor`å¯¹è±¡](https://docs.fast.ai/text.data.html#TokenizeProcessor)å°†`tokenizer`ä¸€ä¸ª`Tokenizer`å¯¹è±¡ä½œä¸ºå‚æ•°ã€‚
2. è¯¥[`Tokenizer`å¯¹è±¡](https://docs.fast.ai/text.transform.html#Tokenizer)å°†`tok_func`ä¸€ä¸ª`BaseTokenizer`å¯¹è±¡ä½œä¸ºå‚æ•°ã€‚
3. è¯¥[`BaseTokenizer`å¯¹è±¡](https://docs.fast.ai/text.transform.html#BaseTokenizer)å®ç°`tokenizer(t:str) â†’ List[str]`æ¥å—æ–‡æœ¬`t`å¹¶è¿”å›å…¶ä»¤ç‰Œåˆ—è¡¨çš„åŠŸèƒ½ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆ›å»ºä¸€ä¸ª`TransformersBaseTokenizer`ç»§æ‰¿`BaseTokenizer`å¹¶è¦†ç›–æ–°`tokenizer`å‡½æ•°çš„æ–°ç±»ã€‚

In [12]:

```python
class TransformersBaseTokenizer(BaseTokenizer):
    """Wrapper around PreTrainedTokenizer to be compatible with fast.ai"""
    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):
        self._pretrained_tokenizer = pretrained_tokenizer
        self.max_seq_len = pretrained_tokenizer.max_len
        self.model_type = model_type

    def __call__(self, *args, **kwargs): 
        return self

    def tokenizer(self, t:str) -> List[str]:
        """Limits the maximum sequence length and add the spesial tokens"""
        CLS = self._pretrained_tokenizer.cls_token
        SEP = self._pretrained_tokenizer.sep_token
        if self.model_type in ['roberta']:
            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]
            tokens = [CLS] + tokens + [SEP]
        else:
            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]
            if self.model_type in ['xlnet']:
                tokens = tokens + [SEP] +  [CLS]
            else:
                tokens = [CLS] + tokens + [SEP]
        return tokens
```

In [13]:

```python
transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)
transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)
fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])
```

Downloading: 100%

899k/899k [00:01<00:00, 709kB/s]

Downloading: 100%

456k/456k [00:00<00:00, 1.37MB/s]

åœ¨æ­¤å®ç°ä¸­ï¼Œè¯·æ³¨æ„ä»¥ä¸‹ä¸‰ç‚¹ï¼š

1. ç”±äºæˆ‘ä»¬ä¸ä½¿ç”¨RNNï¼Œå› æ­¤å¿…é¡»å°†åºåˆ—é•¿åº¦é™åˆ¶ä¸ºæ¨¡å‹è¾“å…¥å¤§å°ã€‚
2. å¤§å¤šæ•°æ¨¡å‹éœ€è¦åœ¨åºåˆ—çš„å¼€å¤´å’Œç»“å°¾æ”¾ç½®ç‰¹æ®Šçš„ä»¤ç‰Œã€‚
3. è¯¸å¦‚RoBERTaä¹‹ç±»çš„æŸäº›æ¨¡å‹éœ€è¦ç©ºæ ¼æ¥å¼€å§‹è¾“å…¥å­—ç¬¦ä¸²ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œåº”ä½¿ç”¨`add_prefix_space`è®¾ç½®ä¸ºæ¥è°ƒç”¨ç¼–ç æ–¹æ³•`True`ã€‚

åœ¨ä¸‹é¢ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„5ç§æ¨¡å‹ç±»å‹çš„æ¯ä¸ªé¢„å¤„ç†è¦æ±‚çš„ç®€å†ã€‚æ‚¨è¿˜å¯ä»¥åœ¨æ¯ä¸ªæ¨¡å‹éƒ¨åˆ†çš„[HuggingFaceæ–‡æ¡£](https://huggingface.co/transformers/)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ã€‚

```
bert:       [CLS] + tokens + [SEP] + padding

roberta:    [CLS] + prefix_space + tokens + [SEP] + padding

distilbert: [CLS] + tokens + [SEP] + padding

xlm:        [CLS] + tokens + [SEP] + padding

xlnet:      padding + tokens + [SEP] + [CLS]
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨å®ç°çš„è¿™ä¸€éƒ¨åˆ†ä¸­æ·»åŠ å¡«å……ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œ`fastai`åœ¨åˆ›å»ºæ—¶è‡ªåŠ¨å¯¹å…¶è¿›è¡Œç®¡ç†`DataBunch`ã€‚

### è‡ªå®šä¹‰æ•°å­—åŒ–å™¨

åœ¨`fastai`ï¼Œ[`NumericalizeProcessor`å¯¹è±¡](https://docs.fast.ai/text.data.html#NumericalizeProcessor)é‡‡ç”¨ä½œä¸º`vocab`å‚æ•°ä¸€ä¸ª[`Vocab`å¯¹è±¡](https://docs.fast.ai/text.transform.html#Vocab)ã€‚é€šè¿‡æ­¤åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§é€‚é…Fastaiæ•°å­—åŒ–å™¨çš„æ–¹æ³•ï¼š

1. æ‚¨å¯ä»¥åƒ[Dev Sharmaçš„æ–‡ç« ](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)ï¼ˆç¬¬*1*èŠ‚*ã€‚è®¾ç½®*ä»¤ç‰Œç”Ÿæˆå™¨ï¼‰ä¸­æ‰€è¿°ï¼Œæ£€ç´¢ä»¤ç‰Œåˆ—è¡¨å¹¶åˆ›å»ºä¸€ä¸ª`Vocab`å¯¹è±¡ã€‚
2. åˆ›å»ºä¸€ä¸ªæ–°çš„ç±»`TransformersVocab`ï¼Œä»ç»§æ‰¿`Vocab`å’Œè¦†ç›–`numericalize`å’Œ`textify`åŠŸèƒ½ã€‚

å³ä½¿ç¬¬ä¸€ä¸ªè§£å†³æ–¹æ¡ˆä¼¼ä¹æ›´ç®€å•ï¼Œ`Transformers`ä¹Ÿæ— æ³•ä¸ºæ‰€æœ‰æ¨¡å‹æä¾›ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥æ£€ç´¢å…¶ä»£å¸åˆ—è¡¨ã€‚å› æ­¤ï¼Œæˆ‘å®ç°äº†ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼Œè¯¥è§£å†³æ–¹æ¡ˆé’ˆå¯¹æ¯ç§æ¨¡å‹ç±»å‹è¿è¡Œã€‚å®ƒç”±ä½¿ç”¨çš„åŠŸèƒ½çš„`convert_tokens_to_ids`å’Œ`convert_ids_to_tokens`åˆ†åˆ«åœ¨`numericalize`å’Œ`textify`ã€‚

In [14]:

```python
class TransformersVocab(Vocab):
    def __init__(self, tokenizer: PreTrainedTokenizer):
        super(TransformersVocab, self).__init__(itos = [])
        self.tokenizer = tokenizer
    
    def numericalize(self, t:Collection[str]) -> List[int]:
        "Convert a list of tokens `t` to their ids."
        return self.tokenizer.convert_tokens_to_ids(t)
        #return self.tokenizer.encode(t)

    def textify(self, nums:Collection[int], sep=' ') -> List[str]:
        "Convert a list of `nums` to their tokens."
        nums = np.array(nums).tolist()
        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)
    
    def __getstate__(self):
        return {'itos':self.itos, 'tokenizer':self.tokenizer}

    def __setstate__(self, state:dict):
        self.itos = state['itos']
        self.tokenizer = state['tokenizer']
        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})
```

æ³¨ï¼šè¯¥åŠŸèƒ½`__gestate__`å¹¶`__setstate__`å…è®¸èŒèƒ½[å‡ºå£](https://docs.fast.ai/basic_train.html#Learner.export)å’Œ[load_learner](https://docs.fast.ai/basic_train.html#load_learner)å·¥ä½œæ­£ç¡®åœ°`TransformersVocab`ã€‚

### å®šåˆ¶å¤„ç†å™¨

ç°åœ¨æˆ‘ä»¬æœ‰äº†è‡ªå®šä¹‰**æ ‡è®°å™¨**å’Œ**æ•°å­—åŒ–å™¨**ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºè‡ªå®šä¹‰**å¤„ç†å™¨**ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ­£åœ¨ä¼ é€’`include_bos = False`å’Œ`include_eos = False`é€‰é¡¹ã€‚è¿™æ˜¯å› ä¸º`fastai`é»˜è®¤æƒ…å†µä¸‹ä¼šæ·»åŠ è‡ªå·±çš„ç‰¹æ®Šä»¤ç‰Œï¼Œè¿™ä¼šå¹²æ‰°æˆ‘ä»¬çš„è‡ªå®šä¹‰ä»¤ç‰Œç”Ÿæˆå™¨æ·»åŠ çš„`[CLS]`å’Œ`[SEP]`ä»¤ç‰Œã€‚

In [15]:

```python
transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)
numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)

tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)

transformer_processor = [tokenize_processor, numericalize_processor]
```

## è®¾ç½®æ•°æ®ç»‘å®š

å¯¹äºåˆ›å»ºDataBunchï¼Œæ‚¨å¿…é¡»æ³¨æ„å°†Processorå‚æ•°è®¾ç½®ä¸ºæ–°çš„è‡ªå®šä¹‰å¤„ç†å™¨ï¼Œ`transformer_processor`å¹¶æ­£ç¡®ç®¡ç†å¡«å……ã€‚

å¦‚HuggingFaceæ–‡æ¡£ä¸­æ‰€è¿°ï¼ŒBERTï¼ŒRoBERTaï¼ŒXLMå’ŒDistilBERTæ˜¯å…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤é€šå¸¸å»ºè®®åœ¨å³ä¾§è€Œä¸æ˜¯å·¦ä¾§å¡«å……è¾“å…¥ã€‚å¯¹äºXLNETï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨å³ä¾§æˆ–å·¦ä¾§å¡«å……è¾“å…¥ã€‚

In [16]:

```python
pad_first = bool(model_type in ['xlnet'])
pad_idx = transformer_tokenizer.pad_token_id
```

In [17]:

```python
tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')
print(tokens)
ids = transformer_tokenizer.convert_tokens_to_ids(tokens)
print(ids)
transformer_tokenizer.convert_ids_to_tokens(ids)
['Sal', 'ut', 'Ä c', 'Ä est', 'Ä mo', 'i', ',', 'Ä Hello', 'Ä it', 'Ä s', 'Ä me']
[18111, 1182, 740, 3304, 7458, 118, 6, 20920, 24, 579, 162]
```

Out[17]:

```
['Sal', 'ut', 'Ä c', 'Ä est', 'Ä mo', 'i', ',', 'Ä Hello', 'Ä it', 'Ä s', 'Ä me']
```

åˆ›å»ºDataBunchæœ‰å¤šç§æ–¹æ³•ï¼Œåœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[æ•°æ®å—API](https://docs.fast.ai/data_block.html#The-data-block-API)ï¼Œå®ƒæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚

In [18]:

```python
databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)
             .split_by_rand_pct(0.1,seed=seed)
             .label_from_df(cols= 'Sentiment')
             .add_test(test)
             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))
```

æ£€æŸ¥æ‰¹å¤„ç†å’Œä»¤ç‰Œç”Ÿæˆå™¨ï¼š

In [19]:

```python
print('[CLS] token :', transformer_tokenizer.cls_token)
print('[SEP] token :', transformer_tokenizer.sep_token)
print('[PAD] token :', transformer_tokenizer.pad_token)
databunch.show_batch()

```
```
[CLS] token : <s>
[SEP] token : </s>
[PAD] token : <pad>
    
| text                                                         | target |
| :----------------------------------------------------------- | :----- |
| <s> Ä - L RB - Ä City Ä - RR B - Ä reminds Ä us Ä how Ä realistically Ä nuanced Ä a Ä Robert Ä De Ä N iro Ä performance Ä can Ä be Ä when Ä he Ä is Ä not Ä more Ä luc r atively Ä engaged Ä in Ä the Ä shameless Ä self - car ic ature Ä of Ä ` Ä Analy ze Ä This Ä ' Ä - L RB - Ä 1999 Ä - RR B - Ä and Ä ` Ä Analy ze Ä That Ä , Ä ' Ä promised Ä - L RB - Ä or Ä threatened Ä - | 3      |
| <s> Ä The Ä real Ä triumph s Ä in Ä Ig by Ä come Ä from Ä Philippe Ä , Ä who Ä makes Ä Oliver Ä far Ä more Ä interesting Ä than Ä the Ä character Ä ' s Ä lines Ä would Ä suggest Ä , Ä and Ä Sar andon Ä , Ä who Ä could Ä n 't Ä be Ä better Ä as Ä a Ä cruel Ä but Ä weird ly Ä lik able Ä WAS P Ä mat ron Ä . </s> | 3      |
| <s> Ä Parker Ä should Ä be Ä comm ended Ä for Ä taking Ä a Ä fresh Ä approach Ä to Ä familiar Ä material Ä , Ä but Ä his Ä determination Ä to Ä remain Ä true Ä to Ä the Ä original Ä text Ä leads Ä him Ä to Ä adopt Ä a Ä somewhat Ä man nered Ä tone Ä ... Ä that Ä ultimately Ä dull s Ä the Ä human Ä tragedy Ä at Ä the Ä story Ä ' s Ä core </s> | 2      |
| <s> Ä It Ä ' s Ä a Ä long Ä way Ä from Ä Orwell Ä ' s Ä dark Ä , Ä intelligent Ä warning Ä cry Ä - L RB - Ä 1984 Ä - RR B - Ä to Ä the Ä empty Ä stud Ä knock about Ä of Ä Equ ilibrium Ä , Ä and Ä what Ä once Ä was Ä conviction Ä is Ä now Ä affect ation Ä . </s> | 1      |
| <s> Ä A Ä different Ä and Ä emotionally Ä reserved Ä type Ä of Ä survival Ä story Ä -- Ä a Ä film Ä less Ä about Ä ref ract ing Ä all Ä of Ä World Ä War Ä II Ä through Ä the Ä specific Ä conditions Ä of Ä one Ä man Ä , Ä and Ä more Ä about Ä that Ä man Ä lost Ä in Ä its Ä midst Ä . </s> | 3      |
```

æ£€æŸ¥æ‰¹å¤„ç†å’Œæ•°å­—åŒ–å™¨ï¼š

In [20]:

```python
print('[CLS] id :', transformer_tokenizer.cls_token_id)
print('[SEP] id :', transformer_tokenizer.sep_token_id)
print('[PAD] id :', pad_idx)
test_one_batch = databunch.one_batch()[0]
print('Batch shape : ',test_one_batch.shape)
print(test_one_batch)

```
```
[CLS] id : 0
[SEP] id : 2
[PAD] id : 1
Batch shape :  torch.Size([16, 79])
tensor([[    0,   111,   574,  ...,    76,   479,     2],
        [    0,    33,     7,  ...,     1,     1,     1],
        [    0,   318,    47,  ...,     1,     1,     1],
        ...,
        [    0,     5,  2156,  ...,     1,     1,     1],
        [    0,    33, 30291,  ...,     1,     1,     1],
        [    0, 45518, 10730,  ...,     1,     1,     1]])
```
    
### å®šåˆ¶æ¨¡å‹

å¦‚æ‰€æåˆ°çš„[åœ¨è¿™é‡Œ](https://github.com/huggingface/transformers#models-always-output-tuples)ï¼Œæ¯ä¸€ä¸ªæ¨¡å‹çš„æ­£å‘æ–¹æ³•æ€»æ˜¯è¾“å‡ºä¸€ä¸ª`tuple`å…·æœ‰å–å†³äºæ¨¡å‹ä¸­çš„å„ç§å…ƒä»¶å’Œé…ç½®å‚æ•°ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªå¸Œæœ›è®¿é—®logitsã€‚è®¿é—®å®ƒä»¬çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

In [21]:

```python
# defining our model architecture 
class CustomTransformerModel(nn.Module):
    def __init__(self, transformer_model: PreTrainedModel):
        super(CustomTransformerModel,self).__init__()
        self.transformer = transformer_model
        
    def forward(self, input_ids, attention_mask=None):
        
        # attention_mask
        # Mask to avoid performing attention on padding token indices.
        # Mask values selected in ``[0, 1]``:
        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.
        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) 
        
        logits = self.transformer(input_ids,
                                  attention_mask = attention_mask)[0]   
        return logits
```

ä¸ºäº†ä½¿æˆ‘ä»¬çš„å˜å‹å™¨é€‚åº”å¤šç±»åˆ†ç±»ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç²¾ç¡®æ ‡è®°çš„æ•°é‡ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹configå®ä¾‹ï¼Œä¹Ÿå¯ä»¥åƒ[Keita Kuritaçš„æ–‡ç« ](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)ï¼ˆéƒ¨åˆ†ï¼š*Initialize the Learner*ï¼‰ä¸­çš„`num_labels`å‚æ•°é‚£æ ·è¿›è¡Œä¿®æ”¹ã€‚

In [22]:

```python
config = config_class.from_pretrained(pretrained_model_name)
config.num_labels = 5
config.use_bfloat16 = use_fp16
print(config)
```

Downloading: 100%

524/524 [00:00<00:00, 881B/s]

```python
RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 5,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}
```

In [23]:

```python
transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)
# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)

custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)
```

Downloading: 100%

501M/501M [00:13<00:00, 36.2MB/s]

## å­¦ä¹ è€…ï¼šè‡ªå®šä¹‰ä¼˜åŒ–å™¨/è‡ªå®šä¹‰æŒ‡æ ‡

åœ¨ä¸­`pytorch-transformers`ï¼ŒHuggingFaceå®ç°äº†ä¸¤ä¸ªç‰¹å®šçš„ä¼˜åŒ–å™¨-BertAdamå’ŒOpenAIAdam-å·²ç”±å•ä¸ªAdamWä¼˜åŒ–å™¨ä»£æ›¿ã€‚è¯¥ä¼˜åŒ–å™¨ä¸Pytorch Adamä¼˜åŒ–å™¨Apiç›¸åŒ¹é…ï¼Œå› æ­¤ï¼Œå°†å…¶é›†æˆåˆ°ä¸­å˜å¾—å¾ˆç®€å•`fastai`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¦é‡ç°BertAdamçš„ç‰¹å®šè¡Œä¸ºï¼Œå¿…é¡»è®¾ç½®`correct_bias = False`ã€‚

In [24]:

```python
from fastai.callbacks import *
from transformers import AdamW
from functools import partial

CustomAdamW = partial(AdamW, correct_bias=False)

learner = Learner(databunch, 
                  custom_transformer_model, 
                  opt_func = CustomAdamW, 
                  metrics=[accuracy, error_rate])

# Show graph of learner stats and metrics after each epoch.
learner.callbacks.append(ShowGraph(learner))

# Put learn in FP16 precision mode. --> Seems to not working
if use_fp16: learner = learner.to_fp16()
```

## åŒºåˆ†æ€§å¾®è°ƒå’Œé€æ­¥è§£å†»ï¼ˆå¯é€‰ï¼‰

è¦ä½¿ç”¨**åˆ¤åˆ«å±‚è®­ç»ƒ**å’Œ**é€æ­¥è§£å†»**ï¼Œ`fastai`æä¾›äº†ä¸€ç§å·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥å°†ç»“æ„æ¨¡å‹â€œæ‹†åˆ†â€ä¸ºç»„ã€‚çš„æŒ‡ä»¤æ¥æ‰§è¡Œâ€œæ‹†åˆ†â€æ˜¯fastaiæ–‡æ¡£ä¸­æè¿°[è¿™é‡Œ](https://docs.fast.ai/basic_train.html#Discriminative-layer-training)ã€‚

ä¸å¹¸çš„æ˜¯ï¼Œæ¨¡å‹æ¶æ„å·®å¼‚å¤ªå¤§ï¼Œæ— æ³•åˆ›å»ºå¯ä»¥ä»¥æ–¹ä¾¿çš„æ–¹å¼â€œæ‹†åˆ†â€æ‰€æœ‰æ¨¡å‹ç±»å‹çš„ç‹¬ç‰¹é€šç”¨å‡½æ•°ã€‚å› æ­¤ï¼Œæ‚¨å°†å¿…é¡»ä¸ºæ¯ä¸ªä¸åŒçš„æ¨¡å‹ä½“ç³»ç»“æ„å®ç°è‡ªå®šä¹‰â€œæ‹†åˆ†â€ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨RobBERTaæ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡è§‚å¯Ÿä»–çš„ä½“ç³»ç»“æ„`print(learner.model)`ã€‚

In [25]:

```python
print(learner.model)
```
```
CustomTransformerModel(
  (transformer): RobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=5, bias=True)
    )
  )
)
```

æˆ‘ä»¬å¯ä»¥å†³å®šå°†æ¨¡å‹åˆ†ä¸º14ä¸ªå—ï¼š

- 1åµŒå…¥
- 12å˜å‹å™¨
- 1ä¸ªåˆ†ç±»å™¨

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼æ‹†åˆ†æ¨¡å‹ï¼š

In [26]:

```python
# For DistilBERT
# list_layers = [learner.model.transformer.distilbert.embeddings,
#                learner.model.transformer.distilbert.transformer.layer[0],
#                learner.model.transformer.distilbert.transformer.layer[1],
#                learner.model.transformer.distilbert.transformer.layer[2],
#                learner.model.transformer.distilbert.transformer.layer[3],
#                learner.model.transformer.distilbert.transformer.layer[4],
#                learner.model.transformer.distilbert.transformer.layer[5],
#                learner.model.transformer.pre_classifier]

# For xlnet-base-cased
# list_layers = [learner.model.transformer.transformer.word_embedding,
#               learner.model.transformer.transformer.layer[0],
#               learner.model.transformer.transformer.layer[1],
#               learner.model.transformer.transformer.layer[2],
#               learner.model.transformer.transformer.layer[3],
#               learner.model.transformer.transformer.layer[4],
#               learner.model.transformer.transformer.layer[5],
#               learner.model.transformer.transformer.layer[6],
#               learner.model.transformer.transformer.layer[7],
#               learner.model.transformer.transformer.layer[8],
#               learner.model.transformer.transformer.layer[9],
#               learner.model.transformer.transformer.layer[10],
#               learner.model.transformer.transformer.layer[11],
#               learner.model.transformer.sequence_summary]

# For roberta-base
list_layers = [learner.model.transformer.roberta.embeddings,
              learner.model.transformer.roberta.encoder.layer[0],
              learner.model.transformer.roberta.encoder.layer[1],
              learner.model.transformer.roberta.encoder.layer[2],
              learner.model.transformer.roberta.encoder.layer[3],
              learner.model.transformer.roberta.encoder.layer[4],
              learner.model.transformer.roberta.encoder.layer[5],
              learner.model.transformer.roberta.encoder.layer[6],
              learner.model.transformer.roberta.encoder.layer[7],
              learner.model.transformer.roberta.encoder.layer[8],
              learner.model.transformer.roberta.encoder.layer[9],
              learner.model.transformer.roberta.encoder.layer[10],
              learner.model.transformer.roberta.encoder.layer[11],
              learner.model.transformer.roberta.pooler]
```

Check groups :

In [27]:

```python
learner.split(list_layers)
num_groups = len(learner.layer_groups)
print('Learner split in',num_groups,'groups')
print(learner.layer_groups)
```
```
Learner split in 14 groups
[Sequential(
  (0): Embedding(50265, 768, padding_idx=1)
  (1): Embedding(514, 768, padding_idx=1)
  (2): Embedding(1, 768)
  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (4): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Linear(in_features=768, out_features=768, bias=True)
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=768, bias=True)
  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (6): Dropout(p=0.1, inplace=False)
  (7): Linear(in_features=768, out_features=3072, bias=True)
  (8): Linear(in_features=3072, out_features=768, bias=True)
  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (10): Dropout(p=0.1, inplace=False)
), Sequential(
  (0): Linear(in_features=768, out_features=768, bias=True)
  (1): Tanh()
  (2): Linear(in_features=768, out_features=768, bias=True)
  (3): Dropout(p=0.1, inplace=False)
  (4): Linear(in_features=768, out_features=5, bias=True)
)]
```

Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.

## è®­ç»ƒ

ç°åœ¨ï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥ä½¿ç”¨æ‰€æœ‰fastaiå†…ç½®åŠŸèƒ½æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚åƒULMFiTæ–¹æ³•ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œ **å€¾æ–œä¸‰è§’å­¦ä¹ ç‡â€**ï¼Œ**â€œåŒºåˆ†å­¦ä¹ ç‡â€**å¹¶**é€æ¸è§£å†»æ¨¡å‹**ã€‚

In [28]:

```python
learner.save('untrain')
```

In [29]:

```python
seed_all(seed)
learner.load('untrain');
```

å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå†»ç»“é™¤åˆ†ç±»å™¨ä¹‹å¤–çš„æ‰€æœ‰ç»„ï¼š

In [30]:

```python
learner.freeze_to(-1)
```

We check which layer are trainable.

In [31]:

```python
learner.summary()
```

Out[31]:

```
CustomTransformerModel
======================================================================
Layer (type)         Output Shape         Param #    Trainable 
======================================================================
Embedding            [79, 768]            38,603,520 False     
______________________________________________________________________
Embedding            [79, 768]            394,752    False     
______________________________________________________________________
Embedding            [79, 768]            768        False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
Dropout              [12, 79, 79]         0          False     
______________________________________________________________________
Linear               [79, 768]            590,592    False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [79, 3072]           2,362,368  False     
______________________________________________________________________
Linear               [79, 768]            2,360,064  False     
______________________________________________________________________
LayerNorm            [79, 768]            1,536      False     
______________________________________________________________________
Dropout              [79, 768]            0          False     
______________________________________________________________________
Linear               [768]                590,592    True      
______________________________________________________________________
Tanh                 [768]                0          False     
______________________________________________________________________
Linear               [768]                590,592    True      
______________________________________________________________________
Dropout              [768]                0          False     
______________________________________________________________________
Linear               [5]                  3,845      True      
______________________________________________________________________

Total params: 125,240,069
Total trainable params: 1,185,029
Total non-trainable params: 124,055,040
Optimized with 'transformers.optimization.AdamW', correct_bias=False
Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ 
Loss function : FlattenedLoss
======================================================================
Callbacks functions applied 
    ShowGraph
```

å¯¹äº**å€¾æ–œä¸‰è§’å­¦ä¹ ç‡ï¼Œ**æ‚¨å¿…é¡»ä½¿ç”¨è¯¥å‡½æ•°`one_cycle`ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·åœ¨[æ­¤å¤„](https://docs.fast.ai/callbacks.one_cycle.html)æŸ¥çœ‹fastaiæ–‡æ¡£ã€‚

è¦ä½¿ç”¨æˆ‘ä»¬ï¼Œ`one_cycle`æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœ€ä½³çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨æ¥æ‰¾åˆ°è¯¥å­¦ä¹ ç‡ï¼Œå¯ä»¥ä½¿ç”¨æ¥è°ƒç”¨`lr_find`ã€‚

In [32]:

```python
learner.lr_find()
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
```

In [33]:

```python
learner.recorder.plot(skip_end=10,suggestion=True)
Min numerical gradient: 3.63E-03
Min loss divided by 10: 4.37E-04
```

![img](imgs/__results___66_1.png)

æˆ‘ä»¬å°†åœ¨æœ€å°å€¼ä¹‹å‰é€‰æ‹©ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼ä»ä¼šæ”¹å–„ã€‚åœ¨è¿™é‡Œ2x10 ^ -3ä¼¼ä¹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€¼ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†`fit_one_cycle`é€‰æ‹©çš„å­¦ä¹ ç‡ç”¨ä½œæœ€å¤§å­¦ä¹ ç‡ã€‚

In [34]:

```python
learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))
```

| epoch | train_loss | valid_loss | accuracy | error_rate | time  |
| :---- | :--------- | :--------- | :------- | :--------- | :---- |
| 0     | 1.012145   | 0.986139   | 0.600538 | 0.399462   | 03:47 |

![img](imgs/__results___68_1.png)

In [35]:

```python
learner.save('first_cycle')
```

In [36]:

```python
seed_all(seed)
learner.load('first_cycle');
```

ç„¶åï¼Œæˆ‘ä»¬è§£å†»ç¬¬äºŒå±‚å›¾å±‚å¹¶é‡å¤æ“ä½œã€‚

In [37]:

```python
learner.freeze_to(-2)
```

In [38]:

```python
lr = 1e-5
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ‡ç‰‡ä¸ºæ¯ä¸ªå°ç»„åˆ›å»ºå•ç‹¬çš„å­¦ä¹ ç‡ã€‚

In [39]:

```python
learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))
```

| epoch | train_loss | valid_loss | accuracy | error_rate | time  |
| :---- | :--------- | :--------- | :------- | :--------- | :---- |
| 0     | 0.927349   | 0.900878   | 0.636935 | 0.363065   | 04:21 |

![img](imgs/__results___75_1.png)

In [40]:

```python
learner.save('second_cycle')
```

In [41]:

```python
seed_all(seed)
learner.load('second_cycle');
```

In [42]:

```python
learner.freeze_to(-3)
```

In [43]:

```python
learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))
```

| epoch | train_loss | valid_loss | accuracy | error_rate | time  |
| :---- | :--------- | :--------- | :------- | :--------- | :---- |
| 0     | 0.894050   | 0.870450   | 0.648917 | 0.351083   | 04:54 |

![img](imgs/__results___79_1.png)

In [44]:

```python
learner.save('third_cycle')
```

In [45]:

```python
seed_all(seed)
learner.load('third_cycle');
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§£å†»æ‰€æœ‰ç»„ã€‚

In [46]:

```python
learner.unfreeze()
```

In [47]:

```python
learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))
```

 50.00% [1/2 11:27<11:27]

| epoch | train_loss | valid_loss | accuracy | error_rate | time  |
| :---- | :--------- | :--------- | :------- | :--------- | :---- |
| 0     | 0.704150   | 0.710882   | 0.702230 | 0.297770   | 11:26 |



 97.03% [8517/8778 11:05<00:20 0.7110]

![img](imgs/__results___84_1.png)

ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é¢„æµ‹ç¤ºä¾‹ï¼š

In [48]:

```python
learner.predict('This is the best movie of 2020')
```

Out[48]:

```
(Category 4,
 tensor(4),
 tensor([8.4167e-06, 1.0881e-05, 1.2710e-04, 2.3995e-02, 9.7586e-01]))
```

In [49]:

```python
learner.predict('This is the worst movie of 2020')
```

Out[49]:

```
(Category 0,
 tensor(0),
 tensor([9.6016e-01, 3.8789e-02, 9.0164e-04, 6.7663e-05, 8.4127e-05]))
```

## å¯¼å‡º Learner

ä¸ºäº†å¯¼å‡ºå’ŒåŠ è½½Learnerï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

In [50]:

```python
learner.export(file = 'transformer.pkl');
```
```
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CustomTransformerModel. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaForSequenceClassification. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaModel. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaEmbeddings. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaClassificationHead. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
```

In [51]:

```python
path = '/kaggle/working'
export_learner = load_learner(path, file = 'transformer.pkl')
```

å¦‚å‰æ‰€è¿°[åœ¨è¿™é‡Œ](https://docs.fast.ai/basic_train.html#load_learner)ï¼Œä½ å¿…é¡»è¦å°å¿ƒï¼Œæ¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»-å–œæ¬¢`TransformersVocab`-åœ¨æ‰§è¡Œä¹‹å‰ï¼Œé¦–å…ˆå®šä¹‰`load_learner`ã€‚

In [52]:

```python
export_learner.predict('This is the worst movie of 2020')
```

Out[52]:

```
(Category 0,
 tensor(0),
 tensor([9.6016e-01, 3.8789e-02, 9.0164e-04, 6.7663e-05, 8.4127e-05]))
```

## å»ºç«‹é¢„æµ‹

ç°åœ¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œæˆ‘ä»¬å¸Œæœ›ä»æµ‹è¯•æ•°æ®é›†ä¸­ç”Ÿæˆé¢„æµ‹ã€‚

æ­£å¦‚Keita Kuritaçš„[æ–‡ç« ä¸­](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)æ‰€æŒ‡å®šçš„é‚£æ ·ï¼Œç”±äºè¯¥å‡½æ•°`get_preds`é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šæŒ‰é¡ºåºè¿”å›å…ƒç´ ï¼Œå› æ­¤æ‚¨å¿…é¡»å°†å…ƒç´ æŒ‰å…¶æ­£ç¡®é¡ºåºè¿›è¡Œæ’åºã€‚

In [53]:

```python
def get_preds_as_nparray(ds_type) -> np.ndarray:
    """
    the get_preds method does not yield the elements in order by default
    we borrow the code from the RNNLearner to resort the elements into their correct order
    """
    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()
    sampler = [i for i in databunch.dl(ds_type).sampler]
    reverse_sampler = np.argsort(sampler)
    return preds[reverse_sampler, :]

test_preds = get_preds_as_nparray(DatasetType.Test)
```

In [54]:

```python
sample_submission = pd.read_csv(DATA_ROOT / 'sampleSubmission.csv')
sample_submission['Sentiment'] = np.argmax(test_preds,axis=1)
sample_submission.to_csv("predictions.csv", index=False)
```

We check the order.

In [55]:

```python
test.head()
```

Out[55]:

|      | PhraseId | SentenceId | Phrase                                            |
| :--- | :------- | :--------- | :------------------------------------------------ |
| 0    | 156061   | 8545       | An intermittently pleasing but mostly routine ... |
| 1    | 156062   | 8545       | An intermittently pleasing but mostly routine ... |
| 2    | 156063   | 8545       | An                                                |
| 3    | 156064   | 8545       | intermittently pleasing but mostly routine effort |
| 4    | 156065   | 8545       | intermittently pleasing but mostly routine        |

In [56]:

```python
sample_submission.head()
```

Out[56]:

|      | PhraseId | Sentiment |
| :--- | :------- | :-------- |
| 0    | 156061   | 2         |
| 1    | 156062   | 2         |
| 2    | 156063   | 2         |
| 3    | 156064   | 2         |
| 4    | 156065   | 2         |

In [57]:

```python
from IPython.display import HTML

def create_download_link(title = "Download CSV file", filename = "data.csv"):  
    html = '<a href={filename}>{title}</a>'
    html = html.format(title=title,filename=filename)
    return HTML(html)

# create a link to download the dataframe which was saved with .to_csv method
create_download_link(filename='predictions.csv')
```

Out[57]:

[ä¸‹è½½CSVæ–‡ä»¶](https://www.kaggleusercontent.com/kf/29225219/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TNdM0JsHJ7LRosHkxiyEqQ.UiPA4oGYtkl-lKB0MIYe3vL0RALRYBH4m8uGG4Cnqh_p8rmjAMKP5wouPGL50PYdfXKuy9uR0DpyRFDISjOgcT5wer2sVnPJozsZrFRMZZk13isRLh92GColPwfXnNEslVRAnpvfULX5v4R_nQE6ASHY0_9J05bsculpyo2Nxn4.pPmrfJgvhblkD7-Z_TfHrg/predictions.csv)

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†é¢„æµ‹æäº¤ç»™Kaggleï¼åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæ²¡æœ‰è¿‡å¤šåœ°ä½¿ç”¨å‚æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°0.70059çš„å¾—åˆ†ï¼Œè¿™ä½¿æˆ‘ä»¬è¿›å…¥äº†æ’è¡Œæ¦œçš„ç¬¬äº”ä½ï¼

# ç»“è®º

åœ¨æ­¤ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘å°†è¯´æ˜å¦‚ä½•å°†`transformers`åº“ä¸å—æ¬¢è¿çš„`fastai`åº“ç»“åˆåœ¨ä¸€èµ·ã€‚å®ƒæ—¨åœ¨ä½¿æ‚¨äº†è§£åœ¨å“ªé‡ŒæŸ¥æ‰¾å’Œä¿®æ”¹ä¸¤ä¸ªåº“ï¼Œä»¥ä½¿å®ƒä»¬ä¸€èµ·å·¥ä½œã€‚å¯èƒ½åœ°ï¼Œå®ƒå…è®¸æ‚¨ä½¿ç”¨**å€¾æ–œçš„ä¸‰è§’å­¦ä¹ ç‡**ï¼Œ**åŒºåˆ†å­¦ä¹ ç‡**ï¼Œç”šè‡³**é€æ¸è§£å†»**ã€‚å› æ­¤ï¼Œæ‚¨ç”šè‡³æ— éœ€è°ƒæ•´å‚æ•°ï¼Œå°±å¯ä»¥å¿«é€Ÿè·å¾—æœ€æ–°çš„ç»“æœã€‚

ä»Šå¹´ï¼Œå˜å‹å™¨æˆä¸ºNLPçš„é‡è¦å·¥å…·ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºé¢„è®­ç»ƒçš„å˜å‹å™¨æ¶æ„å°†å¾ˆå¿«é›†æˆåˆ°fastaiçš„æœªæ¥ç‰ˆæœ¬ä¸­ã€‚åŒæ—¶ï¼Œæœ¬æ•™ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ã€‚

æˆ‘å¸Œæœ›æ‚¨å–œæ¬¢è¿™ç¯‡ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œå¹¶å‘ç°å®ƒå¾ˆæœ‰ç”¨ã€‚æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œä¸è¦çŠ¹è±«ï¼Œæå‡ºé—®é¢˜æˆ–å»ºè®®ã€‚