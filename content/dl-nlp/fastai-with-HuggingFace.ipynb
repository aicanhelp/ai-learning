{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai with HuggingFace ğŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼šæ­¤å®ç°æ˜¯ä¸­å‹æ–‡ç« [â€œå¸¦æœ‰ğŸ¤—Transformersï¼ˆBERTï¼ŒRoBERTaï¼ŒXLNetï¼ŒXLMï¼ŒDistilBERTï¼‰çš„Fastaiâ€çš„è¡¥å……](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376)ã€‚\n",
    "\n",
    "# ç®€ä»‹ï¼šNLPä¸­çš„è½¬ç§»å­¦ä¹ æ•…äº‹\n",
    "\n",
    "åœ¨2018å¹´åˆï¼ŒJeremy Howardï¼ˆfast.aiçš„å…±åŒåˆ›å§‹äººï¼‰å’ŒSebastian Ruderæ¨å‡ºäº†[ç”¨äºæ–‡æœ¬åˆ†ç±»](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf)çš„[é€šç”¨è¯­è¨€æ¨¡å‹å¾®è°ƒ](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf)ï¼ˆULMFiTï¼‰æ–¹æ³•ã€‚ULMFiTæ˜¯åº”ç”¨äºNLP çš„ç¬¬ä¸€ç§**è½¬ç§»å­¦ä¹ **æ–¹æ³•ã€‚ç»“æœï¼Œé™¤äº†æ˜æ˜¾èƒœè¿‡è®¸å¤šæœ€å…ˆè¿›çš„ä»»åŠ¡å¤–ï¼Œå®ƒè¿˜å…è®¸ä»…ç”¨100ä¸ªå¸¦æ ‡ç­¾çš„ç¤ºä¾‹æ¥åŒ¹é…ç­‰åŒäºä½¿ç”¨100å€ä»¥ä¸Šæ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚\n",
    "\n",
    "æˆ‘ç¬¬ä¸€æ¬¡å¬è¯´ULMFiTæ˜¯åœ¨Jeremy Howardæä¾›çš„[fast.aiè¯¾ç¨‹ä¸­](https://course.fast.ai/videos/?lesson=4)ã€‚ç”±äº`fastai`åº“çš„åŸå› ï¼Œä»–æ¼”ç¤ºäº†ç”¨å‡ è¡Œä»£ç å®ç°å®Œæ•´çš„ULMFitæ–¹æ³•æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚åœ¨ä»–çš„æ¼”ç¤ºä¸­ï¼Œä»–ä½¿ç”¨äº†åœ¨Wikitext-103ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„AWD-LSTMç¥ç»ç½‘ç»œï¼Œå¹¶è¿…é€Ÿè·å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚ä»–è¿˜è§£é‡Šäº†å…³é”®æŠ€æœ¯ï¼ˆä¹Ÿå·²åœ¨ULMFiTä¸­è¿›è¡Œäº†æ¼”ç¤ºï¼‰ï¼Œä»¥å¾®è°ƒâ€œ **åŒºåˆ†å­¦ä¹ ç‡â€**ï¼Œâ€œ **é€æ­¥è§£å†»â€**æˆ–â€œ **å€¾æ–œä¸‰è§’å­¦ä¹ ç‡â€ç­‰æ¨¡å‹**ã€‚\n",
    "\n",
    "è‡ªä»å¼•å…¥ULMFiTä¹‹åï¼Œ**Transfer Learning**åœ¨NLPä¸­å˜å¾—éå¸¸æµè¡Œï¼Œä½†æ˜¯Googleï¼ˆBERTï¼ŒTransformer-XLï¼ŒXLNetï¼‰ï¼ŒFacebookï¼ˆRoBERTaï¼ŒXLMï¼‰ç”šè‡³OpenAIï¼ˆGPTï¼ŒGPT-2ï¼‰éƒ½å¼€å§‹å¯¹å…¶æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨å¾ˆå¤§çš„è¯­æ–™åº“ä¸Šã€‚è¿™æ¬¡ï¼Œä»–ä»¬éƒ½æ²¡æœ‰ä½¿ç”¨AWD-LSTMç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯éƒ½ä½¿ç”¨äº†åŸºäºTransformerçš„æ›´å¼ºå¤§çš„ä½“ç³»ç»“æ„ï¼ˆè¯·[æ³¨æ„ï¼Œè¿™æ˜¯æ‚¨æ‰€éœ€è¦çš„å…¨éƒ¨](https://arxiv.org/abs/1706.03762)ï¼‰ã€‚\n",
    "\n",
    "å°½ç®¡è¿™äº›æ¨¡å‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†æ˜¯`fastai`è¯·ä¸è¦å°†æ‰€æœ‰æ¨¡å‹éƒ½é›†æˆåœ¨ä¸€èµ·ã€‚å¹¸è¿çš„æ˜¯ï¼Œ[HuggingFaceğŸ¤—](https://huggingface.co/)åˆ›å»ºäº†ä¼—æ‰€å‘¨çŸ¥çš„[å˜å‹å™¨åº“](https://github.com/huggingface/transformers)ã€‚è¯¥åº“ä»¥å‰ç§°ä¸º`pytorch-transformers`or `pytorch-pretrained-bert`ï¼Œå®ƒæ±‡é›†äº†40å¤šç§ç»è¿‡é¢„è®­ç»ƒçš„æœ€æ–°NLPæ¨¡å‹ï¼ˆBERTï¼ŒGPT-2ï¼ŒRoBERTaï¼ŒCTRLâ€¦ï¼‰ã€‚è¯¥å®ç°æä¾›äº†æœ‰è¶£çš„å…¶ä»–å®ç”¨ç¨‹åºï¼Œä¾‹å¦‚ä»¤ç‰Œç”Ÿæˆå™¨ï¼Œä¼˜åŒ–å™¨æˆ–è°ƒåº¦ç¨‹åºã€‚\n",
    "\n",
    "è¯¥`transformers`åº“å¯ä»¥æ˜¯è‡ªç»™è‡ªè¶³çš„ï¼Œä½†æ˜¯å°†å…¶åˆå¹¶åˆ°`fastai`åº“ä¸­å¯ä»¥æä¾›ä¸åŠŸèƒ½å¼ºå¤§çš„fastaiå·¥å…·å…¼å®¹çš„æ›´ç®€å•çš„å®ç°ï¼Œä¾‹å¦‚**åŒºåˆ†å­¦ä¹ ç‡**ï¼Œ**é€æ­¥è§£å†»**æˆ–**å€¾æ–œçš„ä¸‰è§’å½¢å­¦ä¹ ç‡**ã€‚è¿™é‡Œçš„ç›®çš„æ˜¯è®©ä»»ä½•äººï¼ˆæ— è®ºæ˜¯ä¸“å®¶è¿˜æ˜¯éä¸“å®¶ï¼‰éƒ½å¯ä»¥è½»æ¾è·å¾—æœ€æ–°ç»“æœï¼Œå¹¶â€œå†æ¬¡ä½¿NLPä¸å†é…·ç‚«â€ã€‚\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå·²ç»åœ¨ä»¥ä¸‹æ–¹é¢æ¼”ç¤ºäº†HuggingFace `transformers`åº“çš„é›†æˆ`fastai`ï¼š\n",
    "\n",
    "- Keita Kuritaçš„æ–‡ç« [â€œä½¿ç”¨Fast AIç²¾ç‚¼BERTçš„æ•™ç¨‹â€](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)ä½¿`pytorch_pretrained_bert`å›¾ä¹¦é¦†ä¸å…¼å®¹`fastai`ã€‚\n",
    "- Dev Sharmaçš„æ–‡ç« [å°†RoBERTaä¸Fastaiä¸€èµ·ç”¨äºNLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)ï¼Œè¿™ä½¿`pytorch_transformers`åº“ä¸å…¼å®¹`fastai`ã€‚\n",
    "\n",
    "å°½ç®¡è¿™äº›æ–‡ç« çš„è´¨é‡å¾ˆé«˜ï¼Œä½†å…¶æ¼”ç¤ºçš„æŸäº›éƒ¨åˆ†ä¸å†ä¸çš„æœ€æ–°ç‰ˆæœ¬å…¼å®¹`transformers`ã€‚\n",
    "\n",
    "# ğŸ›  fastå°†å˜å‹å™¨ä¸Fastaié›†æˆä»¥è¿›è¡Œå¤šç±»åˆ†ç±»\n",
    "\n",
    "åœ¨å¼€å§‹å®æ–½ä¹‹å‰ï¼Œè¯·æ³¨æ„å¯ä»¥ä»¥å¤šç§ä¸åŒæ–¹å¼å®Œæˆ`transformers`å†…éƒ¨é›†æˆ`fastai`ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šæä¾›æœ€é€šç”¨ï¼Œæœ€çµæ´»çš„ç®€å•è§£å†³æ–¹æ¡ˆã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘å°è¯•åœ¨ä¸¤ä¸ªåº“ä¸­è¿›è¡Œæœ€å°‘çš„ä¿®æ”¹ï¼ŒåŒæ—¶ä½¿å…¶ä¸æœ€å¤§æ•°é‡çš„è½¬æ¢å™¨ä½“ç³»ç»“æ„å…¼å®¹ã€‚\n",
    "\n",
    "è¯·æ³¨æ„ï¼Œé™¤äº†æœ¬â€œç¬”è®°æœ¬â€å’Œâ€œ [ä¸­å‹â€æ–‡ç« å¤–](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376)ï¼Œæˆ‘è¿˜åœ¨GitHubä¸Šæä¾›äº†å¦ä¸€ä¸ªç‰ˆæœ¬ï¼ˆTODOæ·»åŠ é“¾æ¥ï¼‰ã€‚\n",
    "\n",
    "## å›¾ä¹¦é¦†å®‰è£…\n",
    "\n",
    "åœ¨å¼€å§‹å®æ–½ä¹‹å‰ï¼Œæ‚¨éœ€è¦å®‰è£…`fastai`å’Œ`transformers`åº“ã€‚ä¸ºæ­¤ï¼Œåªéœ€æŒ‰ç…§[æ­¤å¤„](https://github.com/fastai/fastai/blob/master/README.md#installation)å’Œ[æ­¤å¤„](https://github.com/huggingface/transformers#installation)çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚\n",
    "\n",
    "åœ¨Kaggleä¸­ï¼Œè¯¥`fastai`åº“å·²å®‰è£…ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€è¦å®‰è£…`transformers`ï¼š\n",
    "\n",
    "In [1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path \n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random \n",
    "\n",
    "# fastai\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastaiå’ŒTransformersåº“çš„å½“å‰ç‰ˆæœ¬åˆ†åˆ«ä¸º1.0.58å’Œ2.5.1ã€‚\n",
    "\n",
    "In [3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import transformers\n",
    "print('fastai version :', fastai.__version__)\n",
    "print('transformers version :', transformers.__version__)\n",
    "fastai version : 1.0.58\n",
    "transformers version : 2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¬ taskç¤ºä¾‹ä»»åŠ¡\n",
    "\n",
    "é€‰æ‹©çš„ä»»åŠ¡æ˜¯â€œ [ç”µå½±è¯„è®ºâ€](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview)ä¸Šçš„å¤šç±»æ–‡æœ¬åˆ†ç±»ã€‚\n",
    "\n",
    "å¯¹äºæ¯ä¸ªæ–‡æœ¬ç”µå½±è¯„è®ºï¼Œæ¨¡å‹å¿…é¡»é¢„æµ‹æƒ…æ„Ÿçš„æ ‡ç­¾ã€‚æˆ‘ä»¬è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„åˆ†ç±»ç²¾åº¦ã€‚æƒ…æ„Ÿæ ‡ç­¾æ˜¯ï¼š\n",
    "\n",
    "- 0â†’è´Ÿ\n",
    "- 1â†’æœ‰ç‚¹æ¶ˆæ\n",
    "- 2â†’ä¸­ç«‹\n",
    "- 3â†’æœ‰ç‚¹ç§¯æ\n",
    "- 4â†’æ­£\n",
    "\n",
    "æ•°æ®è¢«åŠ è½½åˆ°`DataFrame`usingä¸­`pandas`ã€‚\n",
    "\n",
    "In [4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n",
    "/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n",
    "/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"..\") / \"/kaggle/input/sentiment-analysis-on-movie-reviews\"\n",
    "train = pd.read_csv(DATA_ROOT / 'train.tsv.zip', sep=\"\\t\")\n",
    "test = pd.read_csv(DATA_ROOT / 'test.tsv.zip', sep=\"\\t\")\n",
    "print(train.shape,test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(156060, 4) (66292, 3)\n",
    "```\n",
    "\n",
    "Out[5]:\n",
    "\n",
    "|      | PhraseId | SentenceId | Phrase                                            | Sentiment |\n",
    "| :--- | :------- | :--------- | :------------------------------------------------ | :-------- |\n",
    "| 0    | 1        | 1          | A series of escapades demonstrating the adage ... | 1         |\n",
    "| 1    | 2        | 1          | A series of escapades demonstrating the adage ... | 2         |\n",
    "| 2    | 3        | 1          | A series                                          | 2         |\n",
    "| 3    | 4        | 1          | A                                                 | 2         |\n",
    "| 4    | 5        | 1          | series                                            | 2         |\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ•°æ®é›†ä¸­æ²¡æœ‰å•ç‹¬çš„ç”µå½±è¯„è®ºï¼Œè€Œæ˜¯ä»ä¸Šä¸‹æ–‡ä¸­å–å‡ºå¹¶åˆ†æˆè¾ƒå°éƒ¨åˆ†çš„çŸ­è¯­ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰æŒ‡å®šçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚\n",
    "\n",
    "## ä¸»å˜å‹å™¨ç±»\n",
    "\n",
    "åœ¨ä¸­`transformers`ï¼Œæ¯ç§æ¨¡å‹æ¶æ„éƒ½ä¸3ç§ä¸»è¦ç±»å‹çš„ç±»åˆ«ç›¸å…³è”ï¼š\n",
    "\n",
    "- ä¸€ä¸ª**æ¨¡å‹ç±»**åˆ°åŠ è½½/å­˜å‚¨ç‰¹å®šé¢„ç³»æ¨¡å‹ã€‚\n",
    "- ä¸€ä¸ª**æ ‡è®°ç”Ÿæˆå™¨ç±»**é¢„å…ˆå¤„ç†çš„æ•°æ®å¹¶ä½¿å…¶ä¸ç‰¹å®šå‹å·å…¼å®¹ã€‚\n",
    "- ä¸€ä¸ª**é…ç½®ç±»ï¼Œ**ç”¨äºåŠ è½½/å­˜å‚¨ç‰¹å®šæ¨¡å‹çš„é…ç½®ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨BERTæ¶æ„è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œä½ å¯ä»¥ä½¿ç”¨[`BertForSequenceClassification`](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)çš„**æ¨¡å‹ç±»**ï¼Œ[`BertTokenizer`](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer)ç”¨äº**æ ‡è®°ç”Ÿæˆå™¨ç±»**ï¼Œå¹¶[`BertConfig`](https://huggingface.co/transformers/model_doc/bert.html#bertconfig)ä¸º**é…ç½®ç±»**ã€‚\n",
    "\n",
    "ä¸ºäº†åœ¨ç±»ä¹‹é—´è½»æ¾åˆ‡æ¢ï¼ˆæ¯ä¸ªç±»éƒ½ä¸ç‰¹å®šçš„æ¨¡å‹ç±»å‹ç›¸å…³ï¼‰ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸ï¼Œè¯¥å­—å…¸å…è®¸é€šè¿‡ä»…æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹ç±»å‹åç§°æ¥åŠ è½½æ­£ç¡®çš„ç±»ã€‚\n",
    "\n",
    "In [6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¨åæ‚¨å°†çœ‹åˆ°ï¼Œè¿™äº›ç±»å…±äº«ä¸€ä¸ªé€šç”¨çš„ç±»æ–¹æ³•`from_pretrained(pretrained_model_name, ...)`ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå‚æ•°`pretrained_model_name`æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¸¦æœ‰è¦åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹/ä»¤ç‰Œ/é…ç½®çš„å¿«æ·æ–¹å¼åç§°ï¼Œä¾‹å¦‚`'bert-base-uncased'`ã€‚æˆ‘ä»¬å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)çš„å˜å‹å™¨æ–‡æ¡£ä¸­æ‰¾åˆ°æ‰€æœ‰å¿«æ·æ–¹å¼åç§°ã€‚\n",
    "\n",
    "In [7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 42\n",
    "use_fp16 = False\n",
    "bs = 16\n",
    "\n",
    "model_type = 'roberta'\n",
    "pretrained_model_name = 'roberta-base'\n",
    "\n",
    "# model_type = 'bert'\n",
    "# pretrained_model_name='bert-base-uncased'\n",
    "\n",
    "# model_type = 'distilbert'\n",
    "# pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "#model_type = 'xlm'\n",
    "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
    "\n",
    "# model_type = 'xlnet'\n",
    "# pretrained_model_name = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°ä¸æ‰€ä½¿ç”¨çš„`pretrained_model_name`ï¼ˆå¿«æ·æ–¹å¼åç§°ï¼‰ç›¸å¯¹åº”çš„å¯ç”¨å€¼`model_type`ã€‚\n",
    "\n",
    "In [9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.pretrained_model_archive_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬`transformers`ä»…å°†åº“ç”¨äºå¤šç±»æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ•™ç¨‹ä»…é›†æˆäº†å·²å®ç°åºåˆ—åˆ†ç±»æ¨¡å‹çš„è½¬æ¢å™¨ä½“ç³»ç»“æ„ã€‚è¿™äº›æ¨¡å‹ç±»å‹æ˜¯ï¼š\n",
    "\n",
    "- BERTï¼ˆæ¥è‡ªGoogleï¼‰\n",
    "- XLNetï¼ˆæ¥è‡ªGoogle / CMUï¼‰\n",
    "- XLMï¼ˆæ¥è‡ªFacebookï¼‰\n",
    "- RoBERTaï¼ˆæ¥è‡ªFacebookï¼‰\n",
    "- DistilBERTï¼ˆæ¥è‡ªHuggingFaceï¼‰\n",
    "\n",
    "ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³èµ°å¾—æ›´è¿œ-é€šè¿‡å®ç°å¦ä¸€ç§ç±»å‹çš„æ¨¡å‹æˆ–NLPä»»åŠ¡-æœ¬æ•™ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ã€‚\n",
    "\n",
    "## å®ç”¨åŠŸèƒ½\n",
    "\n",
    "è®¾ç½®ç§å­ä»¥ç”Ÿæˆéšæœºæ•°çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "In [10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "ä¸ºäº†åŒ¹é…é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»å°†æ¨¡å‹è¾“å…¥åºåˆ—æ ¼å¼åŒ–ä¸ºç‰¹å®šæ ¼å¼ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¿…é¡»å…ˆ**æ ‡è®°åŒ–**ç„¶åæ­£ç¡®**æ•°å­—**åŒ–æ–‡æœ¬ã€‚è¿™é‡Œçš„å›°éš¾åœ¨äºï¼Œæˆ‘ä»¬å°†è¿›è¡Œå¾®è°ƒçš„æ¯ä¸ªé¢„è®­ç»ƒæ¨¡å‹éƒ½éœ€è¦â€Šä¸é¢„è®­ç»ƒéƒ¨åˆ†ä¸­ä½¿ç”¨çš„é¢„å¤„ç†å®Œå…¨ç›¸åŒçš„ç‰¹å®šé¢„å¤„ç†ï¼ˆâ€Š **æ ‡è®°åŒ–**å’Œ**æ•°å­—****åŒ–**ï¼‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œ**tokenizerç±»**ä»ä¸­`transformers`æä¾›äº†ä¸æ¯ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ç›¸å¯¹åº”çš„æ­£ç¡®çš„é¢„å¤„ç†å·¥å…·ã€‚\n",
    "\n",
    "åœ¨è¯¥`fastai`åº“ä¸­ï¼Œåˆ›å»ºæ—¶ä¼šè‡ªåŠ¨å®Œæˆæ•°æ®é¢„å¤„ç†`DataBunch`ã€‚æ­£å¦‚æ‚¨å°†åœ¨`DataBunch`å®ç°ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œ**ä»¤ç‰ŒåŒ–å™¨**å’Œ**æ•°å­—****åŒ–**å™¨ä»¥ä»¥ä¸‹æ ¼å¼åœ¨å¤„ç†å™¨å‚æ•°ä¸­ä¼ é€’ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ†æä¸€ä¸‹å¦‚ä½•å°†`transformers` **æ ‡è®°**å™¨é›†æˆåˆ°`TokenizeProcessor`å‡½æ•°ä¸­ã€‚\n",
    "\n",
    "### è‡ªè®¢åˆ†è¯å™¨\n",
    "\n",
    "è¿™éƒ¨åˆ†å¯èƒ½ä¼šé€ æˆä¸€äº›æ··ä¹±ï¼Œå› ä¸ºè®¸å¤šç±»ç›¸äº’åŒ…è£¹å¹¶ä¸”åç§°ç›¸ä¼¼ã€‚ç»§ç»­ï¼Œå¦‚æœæˆ‘ä»¬ä»”ç»†åœ°çœ‹ä¸€ä¸‹`fastai`å®ç°ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ï¼š\n",
    "\n",
    "1. è¯¥[`TokenizeProcessor`å¯¹è±¡](https://docs.fast.ai/text.data.html#TokenizeProcessor)å°†`tokenizer`ä¸€ä¸ª`Tokenizer`å¯¹è±¡ä½œä¸ºå‚æ•°ã€‚\n",
    "2. è¯¥[`Tokenizer`å¯¹è±¡](https://docs.fast.ai/text.transform.html#Tokenizer)å°†`tok_func`ä¸€ä¸ª`BaseTokenizer`å¯¹è±¡ä½œä¸ºå‚æ•°ã€‚\n",
    "3. è¯¥[`BaseTokenizer`å¯¹è±¡](https://docs.fast.ai/text.transform.html#BaseTokenizer)å®ç°`tokenizer(t:str) â†’ List[str]`æ¥å—æ–‡æœ¬`t`å¹¶è¿”å›å…¶ä»¤ç‰Œåˆ—è¡¨çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆ›å»ºä¸€ä¸ª`TransformersBaseTokenizer`ç»§æ‰¿`BaseTokenizer`å¹¶è¦†ç›–æ–°`tokenizer`å‡½æ•°çš„æ–°ç±»ã€‚\n",
    "\n",
    "In [12]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "            tokens = [CLS] + tokens + [SEP]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "            if self.model_type in ['xlnet']:\n",
    "                tokens = tokens + [SEP] +  [CLS]\n",
    "            else:\n",
    "                tokens = [CLS] + tokens + [SEP]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [13]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading: 100%\n",
    "\n",
    "899k/899k [00:01<00:00, 709kB/s]\n",
    "\n",
    "Downloading: 100%\n",
    "\n",
    "456k/456k [00:00<00:00, 1.37MB/s]\n",
    "\n",
    "åœ¨æ­¤å®ç°ä¸­ï¼Œè¯·æ³¨æ„ä»¥ä¸‹ä¸‰ç‚¹ï¼š\n",
    "\n",
    "1. ç”±äºæˆ‘ä»¬ä¸ä½¿ç”¨RNNï¼Œå› æ­¤å¿…é¡»å°†åºåˆ—é•¿åº¦é™åˆ¶ä¸ºæ¨¡å‹è¾“å…¥å¤§å°ã€‚\n",
    "2. å¤§å¤šæ•°æ¨¡å‹éœ€è¦åœ¨åºåˆ—çš„å¼€å¤´å’Œç»“å°¾æ”¾ç½®ç‰¹æ®Šçš„ä»¤ç‰Œã€‚\n",
    "3. è¯¸å¦‚RoBERTaä¹‹ç±»çš„æŸäº›æ¨¡å‹éœ€è¦ç©ºæ ¼æ¥å¼€å§‹è¾“å…¥å­—ç¬¦ä¸²ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œåº”ä½¿ç”¨`add_prefix_space`è®¾ç½®ä¸ºæ¥è°ƒç”¨ç¼–ç æ–¹æ³•`True`ã€‚\n",
    "\n",
    "åœ¨ä¸‹é¢ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„5ç§æ¨¡å‹ç±»å‹çš„æ¯ä¸ªé¢„å¤„ç†è¦æ±‚çš„ç®€å†ã€‚æ‚¨è¿˜å¯ä»¥åœ¨æ¯ä¸ªæ¨¡å‹éƒ¨åˆ†çš„[HuggingFaceæ–‡æ¡£](https://huggingface.co/transformers/)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ã€‚\n",
    "\n",
    "```\n",
    "bert:       [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n",
    "\n",
    "distilbert: [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "xlm:        [CLS] + tokens + [SEP] + padding\n",
    "\n",
    "xlnet:      padding + tokens + [SEP] + [CLS]\n",
    "```\n",
    "\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨å®ç°çš„è¿™ä¸€éƒ¨åˆ†ä¸­æ·»åŠ å¡«å……ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œ`fastai`åœ¨åˆ›å»ºæ—¶è‡ªåŠ¨å¯¹å…¶è¿›è¡Œç®¡ç†`DataBunch`ã€‚\n",
    "\n",
    "### è‡ªå®šä¹‰æ•°å­—åŒ–å™¨\n",
    "\n",
    "åœ¨`fastai`ï¼Œ[`NumericalizeProcessor`å¯¹è±¡](https://docs.fast.ai/text.data.html#NumericalizeProcessor)é‡‡ç”¨ä½œä¸º`vocab`å‚æ•°ä¸€ä¸ª[`Vocab`å¯¹è±¡](https://docs.fast.ai/text.transform.html#Vocab)ã€‚é€šè¿‡æ­¤åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§é€‚é…Fastaiæ•°å­—åŒ–å™¨çš„æ–¹æ³•ï¼š\n",
    "\n",
    "1. æ‚¨å¯ä»¥åƒ[Dev Sharmaçš„æ–‡ç« ](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)ï¼ˆç¬¬*1*èŠ‚*ã€‚è®¾ç½®*ä»¤ç‰Œç”Ÿæˆå™¨ï¼‰ä¸­æ‰€è¿°ï¼Œæ£€ç´¢ä»¤ç‰Œåˆ—è¡¨å¹¶åˆ›å»ºä¸€ä¸ª`Vocab`å¯¹è±¡ã€‚\n",
    "2. åˆ›å»ºä¸€ä¸ªæ–°çš„ç±»`TransformersVocab`ï¼Œä»ç»§æ‰¿`Vocab`å’Œè¦†ç›–`numericalize`å’Œ`textify`åŠŸèƒ½ã€‚\n",
    "\n",
    "å³ä½¿ç¬¬ä¸€ä¸ªè§£å†³æ–¹æ¡ˆä¼¼ä¹æ›´ç®€å•ï¼Œ`Transformers`ä¹Ÿæ— æ³•ä¸ºæ‰€æœ‰æ¨¡å‹æä¾›ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥æ£€ç´¢å…¶ä»£å¸åˆ—è¡¨ã€‚å› æ­¤ï¼Œæˆ‘å®ç°äº†ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼Œè¯¥è§£å†³æ–¹æ¡ˆé’ˆå¯¹æ¯ç§æ¨¡å‹ç±»å‹è¿è¡Œã€‚å®ƒç”±ä½¿ç”¨çš„åŠŸèƒ½çš„`convert_tokens_to_ids`å’Œ`convert_ids_to_tokens`åˆ†åˆ«åœ¨`numericalize`å’Œ`textify`ã€‚\n",
    "\n",
    "In [14]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
    "\n",
    "    def __setstate__(self, state:dict):\n",
    "        self.itos = state['itos']\n",
    "        self.tokenizer = state['tokenizer']\n",
    "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨ï¼šè¯¥åŠŸèƒ½`__gestate__`å¹¶`__setstate__`å…è®¸èŒèƒ½[å‡ºå£](https://docs.fast.ai/basic_train.html#Learner.export)å’Œ[load_learner](https://docs.fast.ai/basic_train.html#load_learner)å·¥ä½œæ­£ç¡®åœ°`TransformersVocab`ã€‚\n",
    "\n",
    "### å®šåˆ¶å¤„ç†å™¨\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æœ‰äº†è‡ªå®šä¹‰**æ ‡è®°å™¨**å’Œ**æ•°å­—åŒ–å™¨**ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºè‡ªå®šä¹‰**å¤„ç†å™¨**ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ­£åœ¨ä¼ é€’`include_bos = False`å’Œ`include_eos = False`é€‰é¡¹ã€‚è¿™æ˜¯å› ä¸º`fastai`é»˜è®¤æƒ…å†µä¸‹ä¼šæ·»åŠ è‡ªå·±çš„ç‰¹æ®Šä»¤ç‰Œï¼Œè¿™ä¼šå¹²æ‰°æˆ‘ä»¬çš„è‡ªå®šä¹‰ä»¤ç‰Œç”Ÿæˆå™¨æ·»åŠ çš„`[CLS]`å’Œ`[SEP]`ä»¤ç‰Œã€‚\n",
    "\n",
    "In [15]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
    "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
    "\n",
    "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "transformer_processor = [tokenize_processor, numericalize_processor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®¾ç½®æ•°æ®ç»‘å®š\n",
    "\n",
    "å¯¹äºåˆ›å»ºDataBunchï¼Œæ‚¨å¿…é¡»æ³¨æ„å°†Processorå‚æ•°è®¾ç½®ä¸ºæ–°çš„è‡ªå®šä¹‰å¤„ç†å™¨ï¼Œ`transformer_processor`å¹¶æ­£ç¡®ç®¡ç†å¡«å……ã€‚\n",
    "\n",
    "å¦‚HuggingFaceæ–‡æ¡£ä¸­æ‰€è¿°ï¼ŒBERTï¼ŒRoBERTaï¼ŒXLMå’ŒDistilBERTæ˜¯å…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤é€šå¸¸å»ºè®®åœ¨å³ä¾§è€Œä¸æ˜¯å·¦ä¾§å¡«å……è¾“å…¥ã€‚å¯¹äºXLNETï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨å³ä¾§æˆ–å·¦ä¾§å¡«å……è¾“å…¥ã€‚\n",
    "\n",
    "In [16]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_first = bool(model_type in ['xlnet'])\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [17]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\n",
    "print(tokens)\n",
    "ids = transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "transformer_tokenizer.convert_ids_to_tokens(ids)\n",
    "['Sal', 'ut', 'Ä c', 'Ä est', 'Ä mo', 'i', ',', 'Ä Hello', 'Ä it', 'Ä s', 'Ä me']\n",
    "[18111, 1182, 740, 3304, 7458, 118, 6, 20920, 24, 579, 162]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[17]:\n",
    "\n",
    "```\n",
    "['Sal', 'ut', 'Ä c', 'Ä est', 'Ä mo', 'i', ',', 'Ä Hello', 'Ä it', 'Ä s', 'Ä me']\n",
    "```\n",
    "\n",
    "åˆ›å»ºDataBunchæœ‰å¤šç§æ–¹æ³•ï¼Œåœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[æ•°æ®å—API](https://docs.fast.ai/data_block.html#The-data-block-API)ï¼Œå®ƒæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚\n",
    "\n",
    "In [18]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)\n",
    "             .split_by_rand_pct(0.1,seed=seed)\n",
    "             .label_from_df(cols= 'Sentiment')\n",
    "             .add_test(test)\n",
    "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ£€æŸ¥æ‰¹å¤„ç†å’Œä»¤ç‰Œç”Ÿæˆå™¨ï¼š\n",
    "\n",
    "In [19]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
    "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
    "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
    "databunch.show_batch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[CLS] token : <s>\n",
    "[SEP] token : </s>\n",
    "[PAD] token : <pad>\n",
    "    \n",
    "| text                                                         | target |\n",
    "| :----------------------------------------------------------- | :----- |\n",
    "| <s> Ä - L RB - Ä City Ä - RR B - Ä reminds Ä us Ä how Ä realistically Ä nuanced Ä a Ä Robert Ä De Ä N iro Ä performance Ä can Ä be Ä when Ä he Ä is Ä not Ä more Ä luc r atively Ä engaged Ä in Ä the Ä shameless Ä self - car ic ature Ä of Ä ` Ä Analy ze Ä This Ä ' Ä - L RB - Ä 1999 Ä - RR B - Ä and Ä ` Ä Analy ze Ä That Ä , Ä ' Ä promised Ä - L RB - Ä or Ä threatened Ä - | 3      |\n",
    "| <s> Ä The Ä real Ä triumph s Ä in Ä Ig by Ä come Ä from Ä Philippe Ä , Ä who Ä makes Ä Oliver Ä far Ä more Ä interesting Ä than Ä the Ä character Ä ' s Ä lines Ä would Ä suggest Ä , Ä and Ä Sar andon Ä , Ä who Ä could Ä n 't Ä be Ä better Ä as Ä a Ä cruel Ä but Ä weird ly Ä lik able Ä WAS P Ä mat ron Ä . </s> | 3      |\n",
    "| <s> Ä Parker Ä should Ä be Ä comm ended Ä for Ä taking Ä a Ä fresh Ä approach Ä to Ä familiar Ä material Ä , Ä but Ä his Ä determination Ä to Ä remain Ä true Ä to Ä the Ä original Ä text Ä leads Ä him Ä to Ä adopt Ä a Ä somewhat Ä man nered Ä tone Ä ... Ä that Ä ultimately Ä dull s Ä the Ä human Ä tragedy Ä at Ä the Ä story Ä ' s Ä core </s> | 2      |\n",
    "| <s> Ä It Ä ' s Ä a Ä long Ä way Ä from Ä Orwell Ä ' s Ä dark Ä , Ä intelligent Ä warning Ä cry Ä - L RB - Ä 1984 Ä - RR B - Ä to Ä the Ä empty Ä stud Ä knock about Ä of Ä Equ ilibrium Ä , Ä and Ä what Ä once Ä was Ä conviction Ä is Ä now Ä affect ation Ä . </s> | 1      |\n",
    "| <s> Ä A Ä different Ä and Ä emotionally Ä reserved Ä type Ä of Ä survival Ä story Ä -- Ä a Ä film Ä less Ä about Ä ref ract ing Ä all Ä of Ä World Ä War Ä II Ä through Ä the Ä specific Ä conditions Ä of Ä one Ä man Ä , Ä and Ä more Ä about Ä that Ä man Ä lost Ä in Ä its Ä midst Ä . </s> | 3      |\n",
    "```\n",
    "\n",
    "æ£€æŸ¥æ‰¹å¤„ç†å’Œæ•°å­—åŒ–å™¨ï¼š\n",
    "\n",
    "In [20]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
    "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
    "print('[PAD] id :', pad_idx)\n",
    "test_one_batch = databunch.one_batch()[0]\n",
    "print('Batch shape : ',test_one_batch.shape)\n",
    "print(test_one_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[CLS] id : 0\n",
    "[SEP] id : 2\n",
    "[PAD] id : 1\n",
    "Batch shape :  torch.Size([16, 79])\n",
    "tensor([[    0,   111,   574,  ...,    76,   479,     2],\n",
    "        [    0,    33,     7,  ...,     1,     1,     1],\n",
    "        [    0,   318,    47,  ...,     1,     1,     1],\n",
    "        ...,\n",
    "        [    0,     5,  2156,  ...,     1,     1,     1],\n",
    "        [    0,    33, 30291,  ...,     1,     1,     1],\n",
    "        [    0, 45518, 10730,  ...,     1,     1,     1]])\n",
    "```\n",
    "    \n",
    "### å®šåˆ¶æ¨¡å‹\n",
    "\n",
    "å¦‚æ‰€æåˆ°çš„[åœ¨è¿™é‡Œ](https://github.com/huggingface/transformers#models-always-output-tuples)ï¼Œæ¯ä¸€ä¸ªæ¨¡å‹çš„æ­£å‘æ–¹æ³•æ€»æ˜¯è¾“å‡ºä¸€ä¸ª`tuple`å…·æœ‰å–å†³äºæ¨¡å‹ä¸­çš„å„ç§å…ƒä»¶å’Œé…ç½®å‚æ•°ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªå¸Œæœ›è®¿é—®logitsã€‚è®¿é—®å®ƒä»¬çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚\n",
    "\n",
    "In [21]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model architecture \n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        # attention_mask\n",
    "        # Mask to avoid performing attention on padding token indices.\n",
    "        # Mask values selected in ``[0, 1]``:\n",
    "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
    "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
    "        \n",
    "        logits = self.transformer(input_ids,\n",
    "                                  attention_mask = attention_mask)[0]   \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†ä½¿æˆ‘ä»¬çš„å˜å‹å™¨é€‚åº”å¤šç±»åˆ†ç±»ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç²¾ç¡®æ ‡è®°çš„æ•°é‡ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹configå®ä¾‹ï¼Œä¹Ÿå¯ä»¥åƒ[Keita Kuritaçš„æ–‡ç« ](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)ï¼ˆéƒ¨åˆ†ï¼š*Initialize the Learner*ï¼‰ä¸­çš„`num_labels`å‚æ•°é‚£æ ·è¿›è¡Œä¿®æ”¹ã€‚\n",
    "\n",
    "In [22]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = 5\n",
    "config.use_bfloat16 = use_fp16\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading: 100%\n",
    "\n",
    "524/524 [00:00<00:00, 881B/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RobertaConfig {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": null,\n",
    "  \"do_sample\": false,\n",
    "  \"eos_token_ids\": null,\n",
    "  \"finetuning_task\": null,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"is_decoder\": false,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1\n",
    "  },\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"length_penalty\": 1.0,\n",
    "  \"max_length\": 20,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_beams\": 1,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"num_labels\": 5,\n",
    "  \"num_return_sequences\": 1,\n",
    "  \"output_attentions\": false,\n",
    "  \"output_hidden_states\": false,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": null,\n",
    "  \"pruned_heads\": {},\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_k\": 50,\n",
    "  \"top_p\": 1.0,\n",
    "  \"torchscript\": false,\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_bfloat16\": false,\n",
    "  \"vocab_size\": 50265\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [23]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
    "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
    "\n",
    "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading: 100%\n",
    "\n",
    "501M/501M [00:13<00:00, 36.2MB/s]\n",
    "\n",
    "## å­¦ä¹ è€…ï¼šè‡ªå®šä¹‰ä¼˜åŒ–å™¨/è‡ªå®šä¹‰æŒ‡æ ‡\n",
    "\n",
    "åœ¨ä¸­`pytorch-transformers`ï¼ŒHuggingFaceå®ç°äº†ä¸¤ä¸ªç‰¹å®šçš„ä¼˜åŒ–å™¨-BertAdamå’ŒOpenAIAdam-å·²ç”±å•ä¸ªAdamWä¼˜åŒ–å™¨ä»£æ›¿ã€‚è¯¥ä¼˜åŒ–å™¨ä¸Pytorch Adamä¼˜åŒ–å™¨Apiç›¸åŒ¹é…ï¼Œå› æ­¤ï¼Œå°†å…¶é›†æˆåˆ°ä¸­å˜å¾—å¾ˆç®€å•`fastai`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¦é‡ç°BertAdamçš„ç‰¹å®šè¡Œä¸ºï¼Œå¿…é¡»è®¾ç½®`correct_bias = False`ã€‚\n",
    "\n",
    "In [24]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "from transformers import AdamW\n",
    "from functools import partial\n",
    "\n",
    "CustomAdamW = partial(AdamW, correct_bias=False)\n",
    "\n",
    "learner = Learner(databunch, \n",
    "                  custom_transformer_model, \n",
    "                  opt_func = CustomAdamW, \n",
    "                  metrics=[accuracy, error_rate])\n",
    "\n",
    "# Show graph of learner stats and metrics after each epoch.\n",
    "learner.callbacks.append(ShowGraph(learner))\n",
    "\n",
    "# Put learn in FP16 precision mode. --> Seems to not working\n",
    "if use_fp16: learner = learner.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŒºåˆ†æ€§å¾®è°ƒå’Œé€æ­¥è§£å†»ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "è¦ä½¿ç”¨**åˆ¤åˆ«å±‚è®­ç»ƒ**å’Œ**é€æ­¥è§£å†»**ï¼Œ`fastai`æä¾›äº†ä¸€ç§å·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥å°†ç»“æ„æ¨¡å‹â€œæ‹†åˆ†â€ä¸ºç»„ã€‚çš„æŒ‡ä»¤æ¥æ‰§è¡Œâ€œæ‹†åˆ†â€æ˜¯fastaiæ–‡æ¡£ä¸­æè¿°[è¿™é‡Œ](https://docs.fast.ai/basic_train.html#Discriminative-layer-training)ã€‚\n",
    "\n",
    "ä¸å¹¸çš„æ˜¯ï¼Œæ¨¡å‹æ¶æ„å·®å¼‚å¤ªå¤§ï¼Œæ— æ³•åˆ›å»ºå¯ä»¥ä»¥æ–¹ä¾¿çš„æ–¹å¼â€œæ‹†åˆ†â€æ‰€æœ‰æ¨¡å‹ç±»å‹çš„ç‹¬ç‰¹é€šç”¨å‡½æ•°ã€‚å› æ­¤ï¼Œæ‚¨å°†å¿…é¡»ä¸ºæ¯ä¸ªä¸åŒçš„æ¨¡å‹ä½“ç³»ç»“æ„å®ç°è‡ªå®šä¹‰â€œæ‹†åˆ†â€ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨RobBERTaæ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡è§‚å¯Ÿä»–çš„ä½“ç³»ç»“æ„`print(learner.model)`ã€‚\n",
    "\n",
    "In [25]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(learner.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CustomTransformerModel(\n",
    "  (transformer): RobertaForSequenceClassification(\n",
    "    (roberta): RobertaModel(\n",
    "      (embeddings): RobertaEmbeddings(\n",
    "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
    "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
    "        (token_type_embeddings): Embedding(1, 768)\n",
    "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "      (encoder): BertEncoder(\n",
    "        (layer): ModuleList(\n",
    "          (0): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (1): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (2): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (3): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (4): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (5): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (6): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (7): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (8): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (9): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (10): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "          (11): BertLayer(\n",
    "            (attention): BertAttention(\n",
    "              (self): BertSelfAttention(\n",
    "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "              (output): BertSelfOutput(\n",
    "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (intermediate): BertIntermediate(\n",
    "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            )\n",
    "            (output): BertOutput(\n",
    "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "              (dropout): Dropout(p=0.1, inplace=False)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (pooler): BertPooler(\n",
    "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (activation): Tanh()\n",
    "      )\n",
    "    )\n",
    "    (classifier): RobertaClassificationHead(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "      (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å†³å®šå°†æ¨¡å‹åˆ†ä¸º14ä¸ªå—ï¼š\n",
    "\n",
    "- 1åµŒå…¥\n",
    "- 12å˜å‹å™¨\n",
    "- 1ä¸ªåˆ†ç±»å™¨\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼æ‹†åˆ†æ¨¡å‹ï¼š\n",
    "\n",
    "In [26]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT\n",
    "# list_layers = [learner.model.transformer.distilbert.embeddings,\n",
    "#                learner.model.transformer.distilbert.transformer.layer[0],\n",
    "#                learner.model.transformer.distilbert.transformer.layer[1],\n",
    "#                learner.model.transformer.distilbert.transformer.layer[2],\n",
    "#                learner.model.transformer.distilbert.transformer.layer[3],\n",
    "#                learner.model.transformer.distilbert.transformer.layer[4],\n",
    "#                learner.model.transformer.distilbert.transformer.layer[5],\n",
    "#                learner.model.transformer.pre_classifier]\n",
    "\n",
    "# For xlnet-base-cased\n",
    "# list_layers = [learner.model.transformer.transformer.word_embedding,\n",
    "#               learner.model.transformer.transformer.layer[0],\n",
    "#               learner.model.transformer.transformer.layer[1],\n",
    "#               learner.model.transformer.transformer.layer[2],\n",
    "#               learner.model.transformer.transformer.layer[3],\n",
    "#               learner.model.transformer.transformer.layer[4],\n",
    "#               learner.model.transformer.transformer.layer[5],\n",
    "#               learner.model.transformer.transformer.layer[6],\n",
    "#               learner.model.transformer.transformer.layer[7],\n",
    "#               learner.model.transformer.transformer.layer[8],\n",
    "#               learner.model.transformer.transformer.layer[9],\n",
    "#               learner.model.transformer.transformer.layer[10],\n",
    "#               learner.model.transformer.transformer.layer[11],\n",
    "#               learner.model.transformer.sequence_summary]\n",
    "\n",
    "# For roberta-base\n",
    "list_layers = [learner.model.transformer.roberta.embeddings,\n",
    "              learner.model.transformer.roberta.encoder.layer[0],\n",
    "              learner.model.transformer.roberta.encoder.layer[1],\n",
    "              learner.model.transformer.roberta.encoder.layer[2],\n",
    "              learner.model.transformer.roberta.encoder.layer[3],\n",
    "              learner.model.transformer.roberta.encoder.layer[4],\n",
    "              learner.model.transformer.roberta.encoder.layer[5],\n",
    "              learner.model.transformer.roberta.encoder.layer[6],\n",
    "              learner.model.transformer.roberta.encoder.layer[7],\n",
    "              learner.model.transformer.roberta.encoder.layer[8],\n",
    "              learner.model.transformer.roberta.encoder.layer[9],\n",
    "              learner.model.transformer.roberta.encoder.layer[10],\n",
    "              learner.model.transformer.roberta.encoder.layer[11],\n",
    "              learner.model.transformer.roberta.pooler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check groups :\n",
    "\n",
    "In [27]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learner.split(list_layers)\n",
    "num_groups = len(learner.layer_groups)\n",
    "print('Learner split in',num_groups,'groups')\n",
    "print(learner.layer_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Learner split in 14 groups\n",
    "[Sequential(\n",
    "  (0): Embedding(50265, 768, padding_idx=1)\n",
    "  (1): Embedding(514, 768, padding_idx=1)\n",
    "  (2): Embedding(1, 768)\n",
    "  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (4): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (6): Dropout(p=0.1, inplace=False)\n",
    "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
    "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  (10): Dropout(p=0.1, inplace=False)\n",
    "), Sequential(\n",
    "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (1): Tanh()\n",
    "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (3): Dropout(p=0.1, inplace=False)\n",
    "  (4): Linear(in_features=768, out_features=5, bias=True)\n",
    ")]\n",
    "```\n",
    "\n",
    "Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.\n",
    "\n",
    "## è®­ç»ƒ\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥ä½¿ç”¨æ‰€æœ‰fastaiå†…ç½®åŠŸèƒ½æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚åƒULMFiTæ–¹æ³•ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œ **å€¾æ–œä¸‰è§’å­¦ä¹ ç‡â€**ï¼Œ**â€œåŒºåˆ†å­¦ä¹ ç‡â€**å¹¶**é€æ¸è§£å†»æ¨¡å‹**ã€‚\n",
    "\n",
    "In [28]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('untrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [29]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('untrain');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå†»ç»“é™¤åˆ†ç±»å™¨ä¹‹å¤–çš„æ‰€æœ‰ç»„ï¼š\n",
    "\n",
    "In [30]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check which layer are trainable.\n",
    "\n",
    "In [31]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[31]:\n",
    "\n",
    "```\n",
    "CustomTransformerModel\n",
    "======================================================================\n",
    "Layer (type)         Output Shape         Param #    Trainable \n",
    "======================================================================\n",
    "Embedding            [79, 768]            38,603,520 False     \n",
    "______________________________________________________________________\n",
    "Embedding            [79, 768]            394,752    False     \n",
    "______________________________________________________________________\n",
    "Embedding            [79, 768]            768        False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "Dropout              [12, 79, 79]         0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            590,592    False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 3072]           2,362,368  False     \n",
    "______________________________________________________________________\n",
    "Linear               [79, 768]            2,360,064  False     \n",
    "______________________________________________________________________\n",
    "LayerNorm            [79, 768]            1,536      False     \n",
    "______________________________________________________________________\n",
    "Dropout              [79, 768]            0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [768]                590,592    True      \n",
    "______________________________________________________________________\n",
    "Tanh                 [768]                0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [768]                590,592    True      \n",
    "______________________________________________________________________\n",
    "Dropout              [768]                0          False     \n",
    "______________________________________________________________________\n",
    "Linear               [5]                  3,845      True      \n",
    "______________________________________________________________________\n",
    "\n",
    "Total params: 125,240,069\n",
    "Total trainable params: 1,185,029\n",
    "Total non-trainable params: 124,055,040\n",
    "Optimized with 'transformers.optimization.AdamW', correct_bias=False\n",
    "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
    "Loss function : FlattenedLoss\n",
    "======================================================================\n",
    "Callbacks functions applied \n",
    "    ShowGraph\n",
    "```\n",
    "\n",
    "å¯¹äº**å€¾æ–œä¸‰è§’å­¦ä¹ ç‡ï¼Œ**æ‚¨å¿…é¡»ä½¿ç”¨è¯¥å‡½æ•°`one_cycle`ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·åœ¨[æ­¤å¤„](https://docs.fast.ai/callbacks.one_cycle.html)æŸ¥çœ‹fastaiæ–‡æ¡£ã€‚\n",
    "\n",
    "è¦ä½¿ç”¨æˆ‘ä»¬ï¼Œ`one_cycle`æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœ€ä½³çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨æ¥æ‰¾åˆ°è¯¥å­¦ä¹ ç‡ï¼Œå¯ä»¥ä½¿ç”¨æ¥è°ƒç”¨`lr_find`ã€‚\n",
    "\n",
    "In [32]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [33]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot(skip_end=10,suggestion=True)\n",
    "Min numerical gradient: 3.63E-03\n",
    "Min loss divided by 10: 4.37E-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/__results___66_1.png)\n",
    "\n",
    "æˆ‘ä»¬å°†åœ¨æœ€å°å€¼ä¹‹å‰é€‰æ‹©ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼ä»ä¼šæ”¹å–„ã€‚åœ¨è¿™é‡Œ2x10 ^ -3ä¼¼ä¹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€¼ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†`fit_one_cycle`é€‰æ‹©çš„å­¦ä¹ ç‡ç”¨ä½œæœ€å¤§å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "In [34]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| epoch | train_loss | valid_loss | accuracy | error_rate | time  |\n",
    "| :---- | :--------- | :--------- | :------- | :--------- | :---- |\n",
    "| 0     | 1.012145   | 0.986139   | 0.600538 | 0.399462   | 03:47 |\n",
    "\n",
    "![img](imgs/__results___68_1.png)\n",
    "\n",
    "In [35]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('first_cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [36]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('first_cycle');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œæˆ‘ä»¬è§£å†»ç¬¬äºŒå±‚å›¾å±‚å¹¶é‡å¤æ“ä½œã€‚\n",
    "\n",
    "In [37]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [38]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ‡ç‰‡ä¸ºæ¯ä¸ªå°ç»„åˆ›å»ºå•ç‹¬çš„å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "In [39]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| epoch | train_loss | valid_loss | accuracy | error_rate | time  |\n",
    "| :---- | :--------- | :--------- | :------- | :--------- | :---- |\n",
    "| 0     | 0.927349   | 0.900878   | 0.636935 | 0.363065   | 04:21 |\n",
    "\n",
    "![img](imgs/__results___75_1.png)\n",
    "\n",
    "In [40]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('second_cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [41]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('second_cycle');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [42]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.freeze_to(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [43]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| epoch | train_loss | valid_loss | accuracy | error_rate | time  |\n",
    "| :---- | :--------- | :--------- | :------- | :--------- | :---- |\n",
    "| 0     | 0.894050   | 0.870450   | 0.648917 | 0.351083   | 04:54 |\n",
    "\n",
    "![img](imgs/__results___79_1.png)\n",
    "\n",
    "In [44]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('third_cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [45]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('third_cycle');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§£å†»æ‰€æœ‰ç»„ã€‚\n",
    "\n",
    "In [46]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [47]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 50.00% [1/2 11:27<11:27]\n",
    "\n",
    "| epoch | train_loss | valid_loss | accuracy | error_rate | time  |\n",
    "| :---- | :--------- | :--------- | :------- | :--------- | :---- |\n",
    "| 0     | 0.704150   | 0.710882   | 0.702230 | 0.297770   | 11:26 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 97.03% [8517/8778 11:05<00:20 0.7110]\n",
    "\n",
    "![img](imgs/__results___84_1.png)\n",
    "\n",
    "ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é¢„æµ‹ç¤ºä¾‹ï¼š\n",
    "\n",
    "In [48]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.predict('This is the best movie of 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[48]:\n",
    "\n",
    "```\n",
    "(Category 4,\n",
    " tensor(4),\n",
    " tensor([8.4167e-06, 1.0881e-05, 1.2710e-04, 2.3995e-02, 9.7586e-01]))\n",
    "```\n",
    "\n",
    "In [49]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.predict('This is the worst movie of 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[49]:\n",
    "\n",
    "```\n",
    "(Category 0,\n",
    " tensor(0),\n",
    " tensor([9.6016e-01, 3.8789e-02, 9.0164e-04, 6.7663e-05, 8.4127e-05]))\n",
    "```\n",
    "\n",
    "## å¯¼å‡º Learner\n",
    "\n",
    "ä¸ºäº†å¯¼å‡ºå’ŒåŠ è½½Learnerï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n",
    "\n",
    "In [50]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "learner.export(file = 'transformer.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CustomTransformerModel. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaForSequenceClassification. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaModel. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaEmbeddings. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RobertaClassificationHead. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "```\n",
    "\n",
    "In [51]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/working'\n",
    "export_learner = load_learner(path, file = 'transformer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚å‰æ‰€è¿°[åœ¨è¿™é‡Œ](https://docs.fast.ai/basic_train.html#load_learner)ï¼Œä½ å¿…é¡»è¦å°å¿ƒï¼Œæ¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»-å–œæ¬¢`TransformersVocab`-åœ¨æ‰§è¡Œä¹‹å‰ï¼Œé¦–å…ˆå®šä¹‰`load_learner`ã€‚\n",
    "\n",
    "In [52]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_learner.predict('This is the worst movie of 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[52]:\n",
    "\n",
    "```\n",
    "(Category 0,\n",
    " tensor(0),\n",
    " tensor([9.6016e-01, 3.8789e-02, 9.0164e-04, 6.7663e-05, 8.4127e-05]))\n",
    "```\n",
    "\n",
    "## å»ºç«‹é¢„æµ‹\n",
    "\n",
    "ç°åœ¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œæˆ‘ä»¬å¸Œæœ›ä»æµ‹è¯•æ•°æ®é›†ä¸­ç”Ÿæˆé¢„æµ‹ã€‚\n",
    "\n",
    "æ­£å¦‚Keita Kuritaçš„[æ–‡ç« ä¸­](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)æ‰€æŒ‡å®šçš„é‚£æ ·ï¼Œç”±äºè¯¥å‡½æ•°`get_preds`é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šæŒ‰é¡ºåºè¿”å›å…ƒç´ ï¼Œå› æ­¤æ‚¨å¿…é¡»å°†å…ƒç´ æŒ‰å…¶æ­£ç¡®é¡ºåºè¿›è¡Œæ’åºã€‚\n",
    "\n",
    "In [53]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the get_preds method does not yield the elements in order by default\n",
    "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
    "    \"\"\"\n",
    "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    return preds[reverse_sampler, :]\n",
    "\n",
    "test_preds = get_preds_as_nparray(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [54]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(DATA_ROOT / 'sampleSubmission.csv')\n",
    "sample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\n",
    "sample_submission.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the order.\n",
    "\n",
    "In [55]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[55]:\n",
    "\n",
    "|      | PhraseId | SentenceId | Phrase                                            |\n",
    "| :--- | :------- | :--------- | :------------------------------------------------ |\n",
    "| 0    | 156061   | 8545       | An intermittently pleasing but mostly routine ... |\n",
    "| 1    | 156062   | 8545       | An intermittently pleasing but mostly routine ... |\n",
    "| 2    | 156063   | 8545       | An                                                |\n",
    "| 3    | 156064   | 8545       | intermittently pleasing but mostly routine effort |\n",
    "| 4    | 156065   | 8545       | intermittently pleasing but mostly routine        |\n",
    "\n",
    "In [56]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[56]:\n",
    "\n",
    "|      | PhraseId | Sentiment |\n",
    "| :--- | :------- | :-------- |\n",
    "| 0    | 156061   | 2         |\n",
    "| 1    | 156062   | 2         |\n",
    "| 2    | 156063   | 2         |\n",
    "| 3    | 156064   | 2         |\n",
    "| 4    | 156065   | 2         |\n",
    "\n",
    "In [57]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[57]:\n",
    "\n",
    "[ä¸‹è½½CSVæ–‡ä»¶](https://www.kaggleusercontent.com/kf/29225219/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TNdM0JsHJ7LRosHkxiyEqQ.UiPA4oGYtkl-lKB0MIYe3vL0RALRYBH4m8uGG4Cnqh_p8rmjAMKP5wouPGL50PYdfXKuy9uR0DpyRFDISjOgcT5wer2sVnPJozsZrFRMZZk13isRLh92GColPwfXnNEslVRAnpvfULX5v4R_nQE6ASHY0_9J05bsculpyo2Nxn4.pPmrfJgvhblkD7-Z_TfHrg/predictions.csv)\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†é¢„æµ‹æäº¤ç»™Kaggleï¼åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæ²¡æœ‰è¿‡å¤šåœ°ä½¿ç”¨å‚æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°0.70059çš„å¾—åˆ†ï¼Œè¿™ä½¿æˆ‘ä»¬è¿›å…¥äº†æ’è¡Œæ¦œçš„ç¬¬äº”ä½ï¼\n",
    "\n",
    "# ç»“è®º\n",
    "\n",
    "åœ¨æ­¤ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘å°†è¯´æ˜å¦‚ä½•å°†`transformers`åº“ä¸å—æ¬¢è¿çš„`fastai`åº“ç»“åˆåœ¨ä¸€èµ·ã€‚å®ƒæ—¨åœ¨ä½¿æ‚¨äº†è§£åœ¨å“ªé‡ŒæŸ¥æ‰¾å’Œä¿®æ”¹ä¸¤ä¸ªåº“ï¼Œä»¥ä½¿å®ƒä»¬ä¸€èµ·å·¥ä½œã€‚å¯èƒ½åœ°ï¼Œå®ƒå…è®¸æ‚¨ä½¿ç”¨**å€¾æ–œçš„ä¸‰è§’å­¦ä¹ ç‡**ï¼Œ**åŒºåˆ†å­¦ä¹ ç‡**ï¼Œç”šè‡³**é€æ¸è§£å†»**ã€‚å› æ­¤ï¼Œæ‚¨ç”šè‡³æ— éœ€è°ƒæ•´å‚æ•°ï¼Œå°±å¯ä»¥å¿«é€Ÿè·å¾—æœ€æ–°çš„ç»“æœã€‚\n",
    "\n",
    "ä»Šå¹´ï¼Œå˜å‹å™¨æˆä¸ºNLPçš„é‡è¦å·¥å…·ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºé¢„è®­ç»ƒçš„å˜å‹å™¨æ¶æ„å°†å¾ˆå¿«é›†æˆåˆ°fastaiçš„æœªæ¥ç‰ˆæœ¬ä¸­ã€‚åŒæ—¶ï¼Œæœ¬æ•™ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ã€‚\n",
    "\n",
    "æˆ‘å¸Œæœ›æ‚¨å–œæ¬¢è¿™ç¯‡ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œå¹¶å‘ç°å®ƒå¾ˆæœ‰ç”¨ã€‚æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œä¸è¦çŠ¹è±«ï¼Œæå‡ºé—®é¢˜æˆ–å»ºè®®ã€‚"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
